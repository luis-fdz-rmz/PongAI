{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from pygame.locals import K_w, K_UP, K_s, K_DOWN, QUIT, K_ESCAPE, KEYDOWN, KEYUP\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pong_ai_game:\n",
    "    def __init__(self, width_screen = 80, height_screen = 60):\n",
    "        self.width_screen = width_screen\n",
    "        self.height_screen = height_screen\n",
    "        self.score = 0\n",
    "        self.reward = [0.0,0.0]\n",
    "        self.screen_color = (35, 35, 35)\n",
    "        self.object_color = (251, 248, 243)\n",
    "        self.game_screen = pygame.display.set_mode((self.width_screen, self.height_screen))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.rect_speed = 2\n",
    "        self.render_game = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.score = 0\n",
    "        self.reward = [0.0,0.0]\n",
    "        self.left_rect = pygame.Rect(0, 7*self.height_screen//16, 2, self.height_screen//8)\n",
    "        self.right_rect = pygame.Rect(self.width_screen - 2, 7*self.height_screen//16, 2, self.height_screen//8)\n",
    "        self.ball_rect = pygame.Rect(self.width_screen//2, self.height_screen//2, 2,2)\n",
    "        self.ball_speed = [pow(-1, np.random.randint(0,2)), pow(-1, np.random.randint(0,2))]\n",
    "        self.render_game = False\n",
    "\n",
    "\n",
    "    def play_step(self, action):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "\n",
    "        paddle, move = action\n",
    "\n",
    "        #Move Up\n",
    "        if move == 0:\n",
    "            if paddle == 1 and self.left_rect.top > 0:\n",
    "                # Move rectangle 1 up by rectangle speed\n",
    "                self.left_rect = self.left_rect.move(0, -self.rect_speed)\n",
    "                if self.left_rect.top < 0:\n",
    "                    self.left_rect.top = 0\n",
    "            elif paddle == 2 and self.right_rect.top > 0:\n",
    "                # Move rectangle 2 up by rectangle speed\n",
    "                self.right_rect = self.right_rect.move(0, -self.rect_speed)\n",
    "                if self.right_rect.top < 0:\n",
    "                    self.right_rect.top = 0\n",
    "        #Move Down\n",
    "        elif move == 1:\n",
    "            if paddle == 1 and self.left_rect.bottom < 600:\n",
    "                # Move rectangle 1 down by rectangle speed\n",
    "                self.left_rect = self.left_rect.move(0, self.rect_speed)\n",
    "                if self.left_rect.bottom > self.height_screen:\n",
    "                    self.left_rect.bottom = self.height_screen\n",
    "            elif paddle == 2 and self.right_rect.bottom < 600:\n",
    "                # Move rectangle 2 down by rectangle speed\n",
    "                self.right_rect = self.right_rect.move(0, self.rect_speed)\n",
    "                if self.right_rect.bottom > self.height_screen:\n",
    "                    self.right_rect.bottom = self.height_screen\n",
    "\n",
    "        self.game_screen.fill(self.screen_color)\n",
    "        self.ball_rect = self.ball_rect.move(self.ball_speed[0],self.ball_speed[1])\n",
    "\n",
    "        self.left_rectangle = pygame.draw.rect(self.game_screen, self.object_color,self.left_rect)\n",
    "        self.right_rectangle = pygame.draw.rect(self.game_screen, self.object_color,self.right_rect)\n",
    "        self.ball_rectangle = pygame.draw.rect(self.game_screen, self.object_color,self.ball_rect)\n",
    "\n",
    "        if self.render_game:\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)\n",
    "\n",
    "        self.ball_collision()\n",
    "\n",
    "        if self.illegal_ball():\n",
    "            return self.reward, True, self.score, self.get_state()\n",
    "\n",
    "        return self.reward, False, self.score, self.get_state()\n",
    "\n",
    "    def illegal_ball(self):\n",
    "        if self.ball_rect.left <= 0 or self.ball_rect.right >= self.width_screen:\n",
    "            if self.ball_rect.left <= 0: self.reward[0] = -10\n",
    "            else: self.reward[1] = -10\n",
    "            return True\n",
    "    def ball_collision(self):\n",
    "\n",
    "        if self.left_rect.right == self.ball_rect.left:\n",
    "            if (self.left_rect.bottom >= self.ball_rect.centery >= self.left_rect.top):\n",
    "                self.ball_speed[0] = 1\n",
    "                self.score += 1\n",
    "                self.reward[0] = 10\n",
    "        elif self.right_rect.left == self.ball_rect.right :\n",
    "            if (self.right_rect.bottom >= self.ball_rect.centery >= self.right_rect.top):\n",
    "                self.ball_speed[0] = -1\n",
    "                self.score += 1\n",
    "                self.reward[1] = 10\n",
    "        if self.ball_rect.top <= 0 or self.ball_rect.bottom >= self.height_screen:\n",
    "            self.ball_speed[1] *= -1\n",
    "\n",
    "        if (self.left_rect.bottom >= self.ball_rect.centery >= self.left_rect.top) and self.ball_speed[0] < 0:\n",
    "            self.reward[0] = 1\n",
    "        elif (self.right_rect.bottom >= self.ball_rect.centery >= self.right_rect.top) and self.ball_speed[0] > 0:\n",
    "            self.reward[1] = 1\n",
    "        else:\n",
    "            if self.ball_speed[0] < 0:\n",
    "                self.reward[0] = -0.1\n",
    "            else:\n",
    "                self.reward[1] = -0.1\n",
    "\n",
    "    def get_state(self):\n",
    "        screen = pygame.surfarray.array3d(self.game_screen)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((40,30)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor()])\n",
    "        screen = preprocess(screen)\n",
    "        return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, lr, input_dims_channel, n_actions):\n",
    "        super(Linear_QNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(4,32,3,1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1)\n",
    "        self.conv3 = nn.Conv2d(64,64,3,1)\n",
    "        self.maxPool = nn.MaxPool2d(3, 1)\n",
    "\n",
    "        # Flatten layer to transition from conv to linear layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the flattened size after convolutional layers\n",
    "        flattened_size = self._get_flattened_size(input_dims_channel)\n",
    "\n",
    "        # Define linear layers\n",
    "        self.linear1 = nn.Linear(flattened_size, 256)\n",
    "        self.linear2 = nn.Linear(256, n_actions)\n",
    "\n",
    "        # Initialize optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Specify device\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _get_flattened_size(self, input_dims_channel):\n",
    "        # Create a dummy input to determine the size after conv layers\n",
    "        dummy_input = torch.zeros(1, 4, 40, 30)  # Assuming input image size is 32x32\n",
    "        dummy_output = self.conv1(dummy_input)\n",
    "        dummy_output = self.maxPool(self.relu(dummy_output))\n",
    "        dummy_output = self.conv2(dummy_output)\n",
    "        dummy_output = self.relu(dummy_output)\n",
    "        dummy_output = self.conv3(dummy_output)\n",
    "        dummy_output = self.relu(dummy_output)\n",
    "        dummy_output = self.flatten(dummy_output)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name = 'model.pth'):\n",
    "        model_folder_path = './model'\n",
    "\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, epsilon, batch_size, rectangle, action_space, max_mem=100_000,\n",
    "                 eps_end=0.01, eps_dec =  0.9995):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.action_space = action_space\n",
    "        self.max_mem = max_mem\n",
    "        self.mem_counter = 0\n",
    "        self.epsilon_end = eps_end\n",
    "        self.epsilon_decay = eps_dec\n",
    "        self.short_model = Linear_QNet(self.lr, 4, len(self.action_space))\n",
    "        self.short_model_weight_counter = 0\n",
    "        self.long_model = Linear_QNet(self.lr, 4, len(self.action_space))\n",
    "        self.long_model_weight_counter = 500\n",
    "        self.state_memory = torch.rand((self.max_mem, 4,40,30), dtype=torch.float32)\n",
    "        self.action_memory = torch.zeros((self.max_mem), dtype=torch.int64)\n",
    "        self.reward_memory = torch.zeros((self.max_mem), dtype=torch.float32)\n",
    "        self.new_state_memory = torch.rand((self.max_mem, 4,40,30), dtype=torch.float32)\n",
    "        self.terminal_memory = torch.zeros((self.max_mem), dtype=torch.int64)\n",
    "        self.show = False\n",
    "        pass\n",
    "\n",
    "\n",
    "    def store_transition(self, state, action,reward, new_state, done):\n",
    "        index = self.mem_counter % self.max_mem\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_counter += 1\n",
    "        self.mem_counter %= self.max_mem\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = observation.unsqueeze(0).to(self.short_model.device)\n",
    "            actions = self.short_model.forward(state).to(self.short_model.device)\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        return action\n",
    "\n",
    "    def get_samples(self):\n",
    "        batch_count = min(self.max_mem, self.mem_counter)\n",
    "        batch_indices = np.random.choice(batch_count, self.batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch_indices].clone().detach().to(self.short_model.device)\n",
    "        actions = self.action_memory[batch_indices].clone().detach().to(self.short_model.device)\n",
    "        rewards = self.reward_memory[batch_indices].clone().detach().to(self.short_model.device)\n",
    "        next_states = self.new_state_memory[batch_indices].clone().detach().to(self.short_model.device)\n",
    "        dones = self.terminal_memory[batch_indices].clone().detach().to(self.short_model.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def learn(self):\n",
    "        # Ensure sufficient memory\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Get samples from memory\n",
    "        states, actions, rewards, next_states, dones = self.get_samples()\n",
    "\n",
    "        # Move tensors to the appropriate device\n",
    "        states = states.to(self.short_model.device)\n",
    "        actions = actions.to(self.short_model.device)\n",
    "        rewards = rewards.to(self.short_model.device)\n",
    "        next_states = next_states.to(self.short_model.device)\n",
    "        dones = dones.to(self.short_model.device)\n",
    "\n",
    "        # Compute current Q values\n",
    "        q_value = self.short_model(states).gather(1, actions.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        # Compute next Q values\n",
    "        q_next = self.long_model(next_states).max(1)[0].to(self.short_model.device).detach()\n",
    "\n",
    "        # Compute target Q values\n",
    "        q_target = rewards + self.gamma * q_next * (1 - dones.int())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.short_model.criterion(q_value, q_target).to(self.short_model.device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        self.short_model.optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        self.short_model.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "run:  0\n",
      "avg: 0.00, eps, 0.980, 1.000, 0\n",
      "run:  1\n",
      "avg: 0.00, eps, 0.980, 0.981, 0\n",
      "run:  2\n",
      "avg: 0.00, eps, 0.961, 0.981, 0\n",
      "run:  3\n",
      "avg: 0.00, eps, 0.961, 0.963, 0\n",
      "run:  4\n",
      "avg: 0.00, eps, 0.942, 0.963, 0\n",
      "run:  5\n",
      "avg: 0.00, eps, 0.942, 0.945, 0\n",
      "run:  6\n",
      "avg: 0.00, eps, 0.942, 0.927, 0\n",
      "run:  7\n",
      "avg: 0.00, eps, 0.923, 0.927, 0\n",
      "run:  8\n",
      "avg: 0.00, eps, 0.923, 0.909, 0\n",
      "run:  9\n",
      "avg: 0.00, eps, 0.923, 0.892, 0\n",
      "run:  10\n",
      "avg: 0.00, eps, 0.905, 0.892, 0\n",
      "run:  11\n",
      "avg: 0.00, eps, 0.887, 0.892, 0\n",
      "run:  12\n",
      "avg: 0.00, eps, 0.887, 0.875, 0\n",
      "run:  13\n",
      "avg: 0.00, eps, 0.869, 0.875, 0\n",
      "run:  14\n",
      "avg: 0.00, eps, 0.869, 0.859, 0\n",
      "run:  15\n",
      "avg: 0.00, eps, 0.852, 0.859, 0\n",
      "run:  16\n",
      "avg: 0.00, eps, 0.835, 0.859, 0\n",
      "run:  17\n",
      "avg: 0.00, eps, 0.819, 0.859, 0\n",
      "run:  18\n",
      "avg: 0.00, eps, 0.802, 0.859, 0\n",
      "run:  19\n",
      "avg: 0.00, eps, 0.787, 0.859, 0\n",
      "run:  20\n",
      "avg: 0.00, eps, 0.771, 0.859, 0\n",
      "run:  21\n",
      "avg: 0.00, eps, 0.756, 0.859, 0\n",
      "run:  22\n",
      "avg: 0.00, eps, 0.756, 0.843, 0\n",
      "run:  23\n",
      "avg: 0.00, eps, 0.756, 0.827, 0\n",
      "run:  24\n",
      "avg: 0.00, eps, 0.756, 0.811, 0\n",
      "run:  25\n",
      "avg: 0.00, eps, 0.756, 0.796, 0\n",
      "run:  26\n",
      "avg: 0.00, eps, 0.741, 0.796, 0\n",
      "run:  27\n",
      "avg: 0.00, eps, 0.741, 0.781, 0\n",
      "run:  28\n",
      "avg: 0.00, eps, 0.726, 0.781, 0\n",
      "run:  29\n",
      "avg: 0.00, eps, 0.712, 0.781, 0\n",
      "run:  30\n",
      "avg: 0.00, eps, 0.698, 0.781, 0\n",
      "run:  31\n",
      "avg: 0.00, eps, 0.698, 0.766, 0\n",
      "run:  32\n",
      "avg: 0.00, eps, 0.684, 0.766, 0\n",
      "run:  33\n",
      "avg: 0.00, eps, 0.670, 0.766, 0\n",
      "run:  34\n",
      "avg: 0.03, eps, 0.645, 0.753, 1\n",
      "run:  35\n",
      "avg: 0.03, eps, 0.632, 0.753, 0\n",
      "run:  36\n",
      "avg: 0.03, eps, 0.620, 0.753, 0\n",
      "run:  37\n",
      "avg: 0.03, eps, 0.608, 0.753, 0\n",
      "run:  38\n",
      "avg: 0.03, eps, 0.596, 0.753, 0\n",
      "run:  39\n",
      "avg: 0.03, eps, 0.584, 0.753, 0\n",
      "run:  40\n",
      "avg: 0.02, eps, 0.584, 0.739, 0\n",
      "run:  41\n",
      "avg: 0.02, eps, 0.584, 0.725, 0\n",
      "run:  42\n",
      "avg: 0.02, eps, 0.572, 0.725, 0\n",
      "run:  43\n",
      "avg: 0.02, eps, 0.572, 0.711, 0\n",
      "run:  44\n",
      "avg: 0.02, eps, 0.572, 0.698, 0\n",
      "run:  45\n",
      "avg: 0.02, eps, 0.572, 0.684, 0\n",
      "run:  46\n",
      "avg: 0.02, eps, 0.572, 0.672, 0\n",
      "run:  47\n",
      "avg: 0.02, eps, 0.572, 0.659, 0\n",
      "run:  48\n",
      "avg: 0.02, eps, 0.561, 0.659, 0\n",
      "run:  49\n",
      "avg: 0.02, eps, 0.561, 0.647, 0\n",
      "run:  50\n",
      "avg: 0.02, eps, 0.550, 0.647, 0\n",
      "run:  51\n",
      "avg: 0.02, eps, 0.539, 0.647, 0\n",
      "run:  52\n",
      "avg: 0.02, eps, 0.528, 0.647, 0\n",
      "run:  53\n",
      "avg: 0.02, eps, 0.518, 0.647, 0\n",
      "run:  54\n",
      "avg: 0.02, eps, 0.508, 0.647, 0\n",
      "run:  55\n",
      "avg: 0.02, eps, 0.508, 0.634, 0\n",
      "run:  56\n",
      "avg: 0.02, eps, 0.497, 0.634, 0\n",
      "run:  57\n",
      "avg: 0.02, eps, 0.497, 0.622, 0\n",
      "run:  58\n",
      "avg: 0.03, eps, 0.479, 0.611, 1\n",
      "run:  59\n",
      "avg: 0.05, eps, 0.461, 0.600, 1\n",
      "run:  60\n",
      "avg: 0.05, eps, 0.461, 0.589, 0\n",
      "run:  61\n",
      "avg: 0.05, eps, 0.461, 0.578, 0\n",
      "run:  62\n",
      "avg: 0.05, eps, 0.461, 0.567, 0\n",
      "run:  63\n",
      "avg: 0.05, eps, 0.452, 0.567, 0\n",
      "run:  64\n",
      "avg: 0.05, eps, 0.452, 0.556, 0\n",
      "run:  65\n",
      "avg: 0.05, eps, 0.452, 0.546, 0\n",
      "run:  66\n",
      "avg: 0.04, eps, 0.443, 0.546, 0\n",
      "run:  67\n",
      "avg: 0.04, eps, 0.443, 0.536, 0\n",
      "run:  68\n",
      "avg: 0.04, eps, 0.443, 0.526, 0\n",
      "run:  69\n",
      "avg: 0.04, eps, 0.434, 0.526, 0\n",
      "run:  70\n",
      "avg: 0.06, eps, 0.418, 0.516, 1\n",
      "run:  71\n",
      "avg: 0.06, eps, 0.418, 0.507, 0\n",
      "run:  72\n",
      "avg: 0.05, eps, 0.410, 0.507, 0\n",
      "run:  73\n",
      "avg: 0.05, eps, 0.410, 0.497, 0\n",
      "run:  74\n",
      "avg: 0.05, eps, 0.402, 0.497, 0\n",
      "run:  75\n",
      "avg: 0.05, eps, 0.394, 0.497, 0\n",
      "run:  76\n",
      "avg: 0.05, eps, 0.386, 0.497, 0\n",
      "run:  77\n",
      "avg: 0.05, eps, 0.386, 0.488, 0\n",
      "run:  78\n",
      "avg: 0.05, eps, 0.386, 0.478, 0\n",
      "run:  79\n",
      "avg: 0.05, eps, 0.386, 0.469, 0\n",
      "run:  80\n",
      "avg: 0.05, eps, 0.378, 0.469, 0\n",
      "run:  81\n",
      "avg: 0.05, eps, 0.371, 0.469, 0\n",
      "run:  82\n",
      "avg: 0.05, eps, 0.371, 0.461, 0\n",
      "run:  83\n",
      "avg: 0.05, eps, 0.363, 0.461, 0\n",
      "run:  84\n",
      "avg: 0.06, eps, 0.350, 0.452, 1\n",
      "run:  85\n",
      "avg: 0.07, eps, 0.343, 0.436, 1\n",
      "run:  86\n",
      "avg: 0.07, eps, 0.343, 0.427, 0\n",
      "run:  87\n",
      "avg: 0.07, eps, 0.336, 0.427, 0\n",
      "run:  88\n",
      "avg: 0.07, eps, 0.336, 0.419, 0\n",
      "run:  89\n",
      "avg: 0.07, eps, 0.330, 0.419, 0\n",
      "run:  90\n",
      "avg: 0.07, eps, 0.323, 0.419, 0\n",
      "run:  91\n",
      "avg: 0.07, eps, 0.317, 0.419, 0\n",
      "run:  92\n",
      "avg: 0.06, eps, 0.317, 0.411, 0\n",
      "run:  93\n",
      "avg: 0.06, eps, 0.317, 0.404, 0\n",
      "run:  94\n",
      "avg: 0.06, eps, 0.311, 0.404, 0\n",
      "run:  95\n",
      "avg: 0.06, eps, 0.311, 0.396, 0\n",
      "run:  96\n",
      "avg: 0.06, eps, 0.304, 0.396, 0\n",
      "run:  97\n",
      "avg: 0.07, eps, 0.293, 0.389, 1\n",
      "run:  98\n",
      "avg: 0.07, eps, 0.293, 0.382, 0\n",
      "run:  99\n",
      "avg: 0.07, eps, 0.293, 0.374, 0\n",
      "run:  100\n",
      "avg: 0.07, eps, 0.287, 0.374, 0\n",
      "run:  101\n",
      "avg: 0.07, eps, 0.287, 0.367, 0\n",
      "run:  102\n",
      "avg: 0.07, eps, 0.282, 0.367, 0\n",
      "run:  103\n",
      "avg: 0.07, eps, 0.276, 0.367, 0\n",
      "run:  104\n",
      "avg: 0.07, eps, 0.271, 0.367, 0\n",
      "run:  105\n",
      "avg: 0.07, eps, 0.265, 0.367, 0\n",
      "run:  106\n",
      "avg: 0.07, eps, 0.265, 0.361, 0\n",
      "run:  107\n",
      "avg: 0.07, eps, 0.265, 0.354, 0\n",
      "run:  108\n",
      "avg: 0.07, eps, 0.265, 0.347, 0\n",
      "run:  109\n",
      "avg: 0.07, eps, 0.260, 0.347, 0\n",
      "run:  110\n",
      "avg: 0.07, eps, 0.255, 0.347, 0\n",
      "run:  111\n",
      "avg: 0.07, eps, 0.255, 0.341, 0\n",
      "run:  112\n",
      "avg: 0.07, eps, 0.255, 0.334, 0\n",
      "run:  113\n",
      "avg: 0.07, eps, 0.255, 0.328, 0\n",
      "run:  114\n",
      "avg: 0.07, eps, 0.250, 0.328, 0\n",
      "run:  115\n",
      "avg: 0.07, eps, 0.250, 0.322, 0\n",
      "run:  116\n",
      "avg: 0.07, eps, 0.250, 0.316, 0\n",
      "run:  117\n",
      "avg: 0.07, eps, 0.250, 0.310, 0\n",
      "run:  118\n",
      "avg: 0.07, eps, 0.250, 0.304, 0\n",
      "run:  119\n",
      "avg: 0.07, eps, 0.250, 0.298, 0\n",
      "run:  120\n",
      "avg: 0.07, eps, 0.250, 0.292, 0\n",
      "run:  121\n",
      "avg: 0.07, eps, 0.245, 0.292, 0\n",
      "run:  122\n",
      "avg: 0.07, eps, 0.240, 0.292, 0\n",
      "run:  123\n",
      "avg: 0.07, eps, 0.240, 0.287, 0\n",
      "run:  124\n",
      "avg: 0.07, eps, 0.240, 0.282, 0\n",
      "run:  125\n",
      "avg: 0.07, eps, 0.235, 0.282, 0\n",
      "run:  126\n",
      "avg: 0.07, eps, 0.231, 0.282, 0\n",
      "run:  127\n",
      "avg: 0.08, eps, 0.222, 0.277, 1\n",
      "run:  128\n",
      "avg: 0.09, eps, 0.214, 0.272, 1\n",
      "run:  129\n",
      "avg: 0.09, eps, 0.214, 0.267, 0\n",
      "run:  130\n",
      "avg: 0.09, eps, 0.209, 0.267, 0\n",
      "run:  131\n",
      "avg: 0.09, eps, 0.205, 0.267, 0\n",
      "run:  132\n",
      "avg: 0.09, eps, 0.205, 0.261, 0\n",
      "run:  133\n",
      "avg: 0.10, eps, 0.201, 0.252, 1\n",
      "run:  134\n",
      "avg: 0.09, eps, 0.201, 0.247, 0\n",
      "run:  135\n",
      "avg: 0.09, eps, 0.197, 0.247, 0\n",
      "run:  136\n",
      "avg: 0.09, eps, 0.197, 0.242, 0\n",
      "run:  137\n",
      "avg: 0.09, eps, 0.197, 0.238, 0\n",
      "run:  138\n",
      "avg: 0.09, eps, 0.197, 0.233, 0\n",
      "run:  139\n",
      "avg: 0.09, eps, 0.197, 0.229, 0\n",
      "run:  140\n",
      "avg: 0.09, eps, 0.194, 0.229, 0\n",
      "run:  141\n",
      "avg: 0.09, eps, 0.194, 0.225, 0\n",
      "run:  142\n",
      "avg: 0.10, eps, 0.186, 0.221, 1\n",
      "run:  143\n",
      "avg: 0.10, eps, 0.183, 0.221, 0\n",
      "run:  144\n",
      "avg: 0.10, eps, 0.179, 0.221, 0\n",
      "run:  145\n",
      "avg: 0.10, eps, 0.179, 0.216, 0\n",
      "run:  146\n",
      "avg: 0.10, eps, 0.179, 0.212, 0\n",
      "run:  147\n",
      "avg: 0.10, eps, 0.179, 0.208, 0\n",
      "run:  148\n",
      "avg: 0.10, eps, 0.179, 0.204, 0\n",
      "run:  149\n",
      "avg: 0.10, eps, 0.175, 0.204, 0\n",
      "run:  150\n",
      "avg: 0.10, eps, 0.172, 0.204, 0\n",
      "run:  151\n",
      "avg: 0.10, eps, 0.169, 0.204, 0\n",
      "run:  152\n",
      "avg: 0.10, eps, 0.169, 0.201, 0\n",
      "run:  153\n",
      "avg: 0.10, eps, 0.165, 0.201, 0\n",
      "run:  154\n",
      "avg: 0.10, eps, 0.162, 0.201, 0\n",
      "run:  155\n",
      "avg: 0.10, eps, 0.162, 0.197, 0\n",
      "run:  156\n",
      "avg: 0.10, eps, 0.159, 0.197, 0\n",
      "run:  157\n",
      "avg: 0.10, eps, 0.156, 0.197, 0\n",
      "run:  158\n",
      "avg: 0.09, eps, 0.153, 0.197, 0\n",
      "run:  159\n",
      "avg: 0.08, eps, 0.153, 0.193, 0\n",
      "run:  160\n",
      "avg: 0.08, eps, 0.153, 0.189, 0\n",
      "run:  161\n",
      "avg: 0.08, eps, 0.153, 0.186, 0\n",
      "run:  162\n",
      "avg: 0.08, eps, 0.149, 0.186, 0\n",
      "run:  163\n",
      "avg: 0.08, eps, 0.149, 0.182, 0\n",
      "run:  164\n",
      "avg: 0.08, eps, 0.149, 0.179, 0\n",
      "run:  165\n",
      "avg: 0.08, eps, 0.147, 0.179, 0\n",
      "run:  166\n",
      "avg: 0.08, eps, 0.147, 0.176, 0\n",
      "run:  167\n",
      "avg: 0.08, eps, 0.147, 0.172, 0\n",
      "run:  168\n",
      "avg: 0.08, eps, 0.144, 0.172, 0\n",
      "run:  169\n",
      "avg: 0.08, eps, 0.144, 0.169, 0\n",
      "run:  170\n",
      "avg: 0.07, eps, 0.141, 0.169, 0\n",
      "run:  171\n",
      "avg: 0.07, eps, 0.141, 0.166, 0\n",
      "run:  172\n",
      "avg: 0.07, eps, 0.138, 0.166, 0\n",
      "run:  173\n",
      "avg: 0.07, eps, 0.135, 0.166, 0\n",
      "run:  174\n",
      "avg: 0.07, eps, 0.135, 0.163, 0\n",
      "run:  175\n",
      "avg: 0.07, eps, 0.135, 0.160, 0\n",
      "run:  176\n",
      "avg: 0.07, eps, 0.133, 0.160, 0\n",
      "run:  177\n",
      "avg: 0.07, eps, 0.130, 0.160, 0\n",
      "run:  178\n",
      "avg: 0.07, eps, 0.130, 0.157, 0\n",
      "run:  179\n",
      "avg: 0.07, eps, 0.127, 0.157, 0\n",
      "run:  180\n",
      "avg: 0.07, eps, 0.127, 0.154, 0\n",
      "run:  181\n",
      "avg: 0.08, eps, 0.123, 0.151, 1\n",
      "run:  182\n",
      "avg: 0.08, eps, 0.120, 0.151, 0\n",
      "run:  183\n",
      "avg: 0.08, eps, 0.118, 0.151, 0\n",
      "run:  184\n",
      "avg: 0.07, eps, 0.118, 0.148, 0\n",
      "run:  185\n",
      "avg: 0.07, eps, 0.113, 0.146, 1\n",
      "run:  186\n",
      "avg: 0.08, eps, 0.109, 0.143, 1\n",
      "run:  187\n",
      "avg: 0.08, eps, 0.107, 0.143, 0\n",
      "run:  188\n",
      "avg: 0.08, eps, 0.105, 0.143, 0\n",
      "run:  189\n",
      "avg: 0.08, eps, 0.103, 0.143, 0\n",
      "run:  190\n",
      "avg: 0.08, eps, 0.101, 0.143, 0\n",
      "run:  191\n",
      "avg: 0.08, eps, 0.099, 0.143, 0\n",
      "run:  192\n",
      "avg: 0.08, eps, 0.097, 0.143, 0\n",
      "run:  193\n",
      "avg: 0.08, eps, 0.095, 0.143, 0\n",
      "run:  194\n",
      "avg: 0.08, eps, 0.095, 0.140, 0\n",
      "run:  195\n",
      "avg: 0.09, eps, 0.091, 0.138, 1\n",
      "run:  196\n",
      "avg: 0.09, eps, 0.090, 0.138, 0\n",
      "run:  197\n",
      "avg: 0.08, eps, 0.088, 0.138, 0\n",
      "run:  198\n",
      "avg: 0.08, eps, 0.086, 0.138, 0\n",
      "run:  199\n",
      "avg: 0.08, eps, 0.086, 0.135, 0\n",
      "run:  200\n",
      "avg: 0.09, eps, 0.083, 0.133, 1\n",
      "run:  201\n",
      "avg: 0.10, eps, 0.080, 0.130, 1\n",
      "run:  202\n",
      "avg: 0.10, eps, 0.078, 0.130, 0\n",
      "run:  203\n",
      "avg: 0.10, eps, 0.078, 0.128, 0\n",
      "run:  204\n",
      "avg: 0.11, eps, 0.075, 0.126, 1\n",
      "run:  205\n",
      "avg: 0.11, eps, 0.074, 0.126, 0\n",
      "run:  206\n",
      "avg: 0.11, eps, 0.074, 0.123, 0\n",
      "run:  207\n",
      "avg: 0.11, eps, 0.074, 0.121, 0\n",
      "run:  208\n",
      "avg: 0.11, eps, 0.072, 0.121, 0\n",
      "run:  209\n",
      "avg: 0.11, eps, 0.072, 0.119, 0\n",
      "run:  210\n",
      "avg: 0.12, eps, 0.070, 0.117, 1\n",
      "run:  211\n",
      "avg: 0.12, eps, 0.068, 0.117, 0\n",
      "run:  212\n",
      "avg: 0.12, eps, 0.067, 0.117, 0\n",
      "run:  213\n",
      "avg: 0.12, eps, 0.066, 0.117, 0\n",
      "run:  214\n",
      "avg: 0.12, eps, 0.064, 0.117, 0\n",
      "run:  215\n",
      "avg: 0.12, eps, 0.063, 0.117, 0\n",
      "run:  216\n",
      "avg: 0.12, eps, 0.062, 0.117, 0\n",
      "run:  217\n",
      "avg: 0.12, eps, 0.061, 0.117, 0\n",
      "run:  218\n",
      "avg: 0.12, eps, 0.059, 0.117, 0\n",
      "run:  219\n",
      "avg: 0.14, eps, 0.057, 0.110, 2\n",
      "run:  220\n",
      "avg: 0.14, eps, 0.057, 0.108, 0\n",
      "run:  221\n",
      "avg: 0.14, eps, 0.057, 0.106, 0\n",
      "run:  222\n",
      "avg: 0.15, eps, 0.055, 0.104, 1\n",
      "run:  223\n",
      "avg: 0.15, eps, 0.054, 0.104, 0\n",
      "run:  224\n",
      "avg: 0.16, eps, 0.053, 0.100, 1\n",
      "run:  225\n",
      "avg: 0.16, eps, 0.052, 0.100, 0\n",
      "run:  226\n",
      "avg: 0.16, eps, 0.051, 0.100, 0\n",
      "run:  227\n",
      "avg: 0.16, eps, 0.050, 0.097, 1\n",
      "run:  228\n",
      "avg: 0.15, eps, 0.049, 0.097, 0\n",
      "run:  229\n",
      "avg: 0.15, eps, 0.048, 0.097, 0\n",
      "run:  230\n",
      "avg: 0.15, eps, 0.047, 0.097, 0\n",
      "run:  231\n",
      "avg: 0.15, eps, 0.046, 0.097, 0\n",
      "run:  232\n",
      "avg: 0.16, eps, 0.044, 0.095, 1\n",
      "run:  233\n",
      "avg: 0.15, eps, 0.043, 0.095, 0\n",
      "run:  234\n",
      "avg: 0.15, eps, 0.043, 0.095, 0\n",
      "run:  235\n",
      "avg: 0.16, eps, 0.041, 0.093, 1\n",
      "run:  236\n",
      "avg: 0.17, eps, 0.039, 0.091, 1\n",
      "run:  237\n",
      "avg: 0.17, eps, 0.039, 0.091, 0\n",
      "run:  238\n",
      "avg: 0.17, eps, 0.038, 0.091, 0\n",
      "run:  239\n",
      "avg: 0.17, eps, 0.038, 0.090, 0\n",
      "run:  240\n",
      "avg: 0.17, eps, 0.038, 0.088, 0\n",
      "run:  241\n",
      "avg: 0.17, eps, 0.037, 0.088, 0\n",
      "run:  242\n",
      "avg: 0.16, eps, 0.036, 0.088, 0\n",
      "run:  243\n",
      "avg: 0.16, eps, 0.036, 0.086, 0\n",
      "run:  244\n",
      "avg: 0.17, eps, 0.035, 0.085, 1\n",
      "run:  245\n",
      "avg: 0.18, eps, 0.034, 0.083, 1\n",
      "run:  246\n",
      "avg: 0.18, eps, 0.033, 0.083, 0\n",
      "run:  247\n",
      "avg: 0.18, eps, 0.032, 0.083, 0\n",
      "run:  248\n",
      "avg: 0.18, eps, 0.032, 0.083, 0\n",
      "run:  249\n",
      "avg: 0.18, eps, 0.032, 0.082, 0\n",
      "run:  250\n",
      "avg: 0.20, eps, 0.031, 0.077, 2\n",
      "run:  251\n",
      "avg: 0.20, eps, 0.031, 0.076, 0\n",
      "run:  252\n",
      "avg: 0.20, eps, 0.031, 0.074, 0\n",
      "run:  253\n",
      "avg: 0.20, eps, 0.031, 0.073, 0\n",
      "run:  254\n",
      "avg: 0.20, eps, 0.031, 0.072, 0\n",
      "run:  255\n",
      "avg: 0.20, eps, 0.031, 0.070, 0\n",
      "run:  256\n",
      "avg: 0.20, eps, 0.031, 0.069, 0\n",
      "run:  257\n",
      "avg: 0.21, eps, 0.030, 0.068, 1\n",
      "run:  258\n",
      "avg: 0.21, eps, 0.029, 0.068, 0\n",
      "run:  259\n",
      "avg: 0.21, eps, 0.028, 0.068, 0\n",
      "run:  260\n",
      "avg: 0.21, eps, 0.028, 0.068, 0\n",
      "run:  261\n",
      "avg: 0.21, eps, 0.027, 0.068, 0\n",
      "run:  262\n",
      "avg: 0.21, eps, 0.027, 0.066, 0\n",
      "run:  263\n",
      "avg: 0.21, eps, 0.027, 0.065, 0\n",
      "run:  264\n",
      "avg: 0.21, eps, 0.027, 0.065, 0\n",
      "run:  265\n",
      "avg: 0.21, eps, 0.027, 0.064, 0\n",
      "run:  266\n",
      "avg: 0.21, eps, 0.027, 0.063, 0\n",
      "run:  267\n",
      "avg: 0.21, eps, 0.027, 0.062, 0\n",
      "run:  268\n",
      "avg: 0.21, eps, 0.026, 0.062, 0\n",
      "run:  269\n",
      "avg: 0.22, eps, 0.025, 0.061, 1\n",
      "run:  270\n",
      "avg: 0.23, eps, 0.024, 0.059, 1\n",
      "run:  271\n",
      "avg: 0.23, eps, 0.024, 0.059, 0\n",
      "run:  272\n",
      "avg: 0.23, eps, 0.024, 0.058, 0\n",
      "run:  273\n",
      "avg: 0.23, eps, 0.023, 0.058, 0\n",
      "run:  274\n",
      "avg: 0.23, eps, 0.023, 0.058, 0\n",
      "run:  275\n",
      "avg: 0.23, eps, 0.022, 0.058, 0\n",
      "run:  276\n",
      "avg: 0.23, eps, 0.022, 0.058, 0\n",
      "run:  277\n",
      "avg: 0.24, eps, 0.021, 0.057, 1\n",
      "run:  278\n",
      "avg: 0.24, eps, 0.021, 0.057, 0\n",
      "run:  279\n",
      "avg: 0.25, eps, 0.020, 0.056, 1\n",
      "run:  280\n",
      "avg: 0.25, eps, 0.020, 0.055, 0\n",
      "run:  281\n",
      "avg: 0.24, eps, 0.020, 0.055, 0\n",
      "run:  282\n",
      "avg: 0.24, eps, 0.020, 0.054, 0\n",
      "run:  283\n",
      "avg: 0.24, eps, 0.019, 0.054, 0\n",
      "run:  284\n",
      "avg: 0.24, eps, 0.019, 0.054, 0\n",
      "run:  285\n",
      "avg: 0.23, eps, 0.019, 0.053, 0\n",
      "run:  286\n",
      "avg: 0.22, eps, 0.019, 0.052, 0\n",
      "run:  287\n",
      "avg: 0.23, eps, 0.018, 0.051, 1\n",
      "run:  288\n",
      "avg: 0.24, eps, 0.017, 0.050, 1\n",
      "run:  289\n",
      "avg: 0.24, eps, 0.017, 0.050, 0\n",
      "run:  290\n",
      "avg: 0.24, eps, 0.017, 0.049, 0\n",
      "run:  291\n",
      "avg: 0.25, eps, 0.017, 0.048, 1\n",
      "run:  292\n",
      "avg: 0.26, eps, 0.016, 0.047, 1\n",
      "run:  293\n",
      "avg: 0.26, eps, 0.016, 0.046, 0\n",
      "run:  294\n",
      "avg: 0.26, eps, 0.016, 0.046, 0\n",
      "run:  295\n",
      "avg: 0.25, eps, 0.015, 0.046, 0\n",
      "run:  296\n",
      "avg: 0.25, eps, 0.015, 0.045, 0\n",
      "run:  297\n",
      "avg: 0.25, eps, 0.015, 0.045, 0\n",
      "run:  298\n",
      "avg: 0.25, eps, 0.015, 0.045, 0\n",
      "run:  299\n",
      "avg: 0.27, eps, 0.014, 0.042, 2\n",
      "run:  300\n",
      "avg: 0.26, eps, 0.014, 0.042, 0\n",
      "run:  301\n",
      "avg: 0.25, eps, 0.014, 0.042, 0\n",
      "run:  302\n",
      "avg: 0.25, eps, 0.014, 0.042, 0\n",
      "run:  303\n",
      "avg: 0.25, eps, 0.013, 0.042, 0\n",
      "run:  304\n",
      "avg: 0.24, eps, 0.013, 0.041, 0\n",
      "run:  305\n",
      "avg: 0.24, eps, 0.013, 0.040, 0\n",
      "run:  306\n",
      "avg: 0.24, eps, 0.013, 0.040, 0\n",
      "run:  307\n",
      "avg: 0.24, eps, 0.013, 0.040, 0\n",
      "run:  308\n",
      "avg: 0.24, eps, 0.013, 0.040, 0\n",
      "run:  309\n",
      "avg: 0.24, eps, 0.012, 0.040, 0\n",
      "run:  310\n",
      "avg: 0.23, eps, 0.012, 0.039, 0\n",
      "run:  311\n",
      "avg: 0.23, eps, 0.012, 0.039, 0\n",
      "run:  312\n",
      "avg: 0.23, eps, 0.012, 0.039, 0\n",
      "run:  313\n",
      "avg: 0.23, eps, 0.012, 0.039, 0\n",
      "run:  314\n",
      "avg: 0.23, eps, 0.012, 0.038, 0\n",
      "run:  315\n",
      "avg: 0.23, eps, 0.012, 0.038, 0\n",
      "run:  316\n",
      "avg: 0.23, eps, 0.012, 0.037, 0\n",
      "run:  317\n",
      "avg: 0.23, eps, 0.012, 0.036, 0\n",
      "run:  318\n",
      "avg: 0.23, eps, 0.012, 0.036, 0\n",
      "run:  319\n",
      "avg: 0.21, eps, 0.011, 0.036, 0\n",
      "run:  320\n",
      "avg: 0.21, eps, 0.011, 0.036, 0\n",
      "run:  321\n",
      "avg: 0.21, eps, 0.011, 0.035, 0\n",
      "run:  322\n",
      "avg: 0.20, eps, 0.011, 0.034, 0\n",
      "run:  323\n",
      "avg: 0.20, eps, 0.011, 0.034, 0\n",
      "run:  324\n",
      "avg: 0.19, eps, 0.011, 0.034, 0\n",
      "run:  325\n",
      "avg: 0.19, eps, 0.011, 0.034, 0\n",
      "run:  326\n",
      "avg: 0.20, eps, 0.010, 0.033, 1\n",
      "run:  327\n",
      "avg: 0.21, eps, 0.010, 0.031, 2\n",
      "run:  328\n",
      "avg: 0.21, eps, 0.010, 0.031, 0\n",
      "run:  329\n",
      "avg: 0.22, eps, 0.010, 0.030, 1\n",
      "run:  330\n",
      "avg: 0.22, eps, 0.010, 0.030, 0\n",
      "run:  331\n",
      "avg: 0.22, eps, 0.010, 0.030, 0\n",
      "run:  332\n",
      "avg: 0.21, eps, 0.010, 0.030, 0\n",
      "run:  333\n",
      "avg: 0.21, eps, 0.010, 0.030, 0\n",
      "run:  334\n",
      "avg: 0.21, eps, 0.010, 0.030, 0\n",
      "run:  335\n",
      "avg: 0.21, eps, 0.010, 0.029, 1\n",
      "run:  336\n",
      "avg: 0.21, eps, 0.010, 0.028, 1\n",
      "run:  337\n",
      "avg: 0.21, eps, 0.010, 0.028, 0\n",
      "run:  338\n",
      "avg: 0.22, eps, 0.010, 0.028, 1\n",
      "run:  339\n",
      "avg: 0.22, eps, 0.010, 0.028, 0\n",
      "run:  340\n",
      "avg: 0.22, eps, 0.010, 0.027, 0\n",
      "run:  341\n",
      "avg: 0.22, eps, 0.010, 0.027, 0\n",
      "run:  342\n",
      "avg: 0.22, eps, 0.010, 0.027, 0\n",
      "run:  343\n",
      "avg: 0.22, eps, 0.010, 0.026, 0\n",
      "run:  344\n",
      "avg: 0.21, eps, 0.010, 0.026, 0\n",
      "run:  345\n",
      "avg: 0.20, eps, 0.010, 0.026, 0\n",
      "run:  346\n",
      "avg: 0.20, eps, 0.010, 0.026, 0\n",
      "run:  347\n",
      "avg: 0.20, eps, 0.010, 0.025, 0\n",
      "run:  348\n",
      "avg: 0.21, eps, 0.010, 0.025, 1\n",
      "run:  349\n",
      "avg: 0.21, eps, 0.010, 0.024, 0\n",
      "run:  350\n",
      "avg: 0.19, eps, 0.010, 0.024, 0\n",
      "run:  351\n",
      "avg: 0.20, eps, 0.010, 0.024, 1\n",
      "run:  352\n",
      "avg: 0.21, eps, 0.010, 0.023, 1\n",
      "run:  353\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  354\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  355\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  356\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  357\n",
      "avg: 0.21, eps, 0.010, 0.023, 1\n",
      "run:  358\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  359\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  360\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  361\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  362\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  363\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  364\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  365\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  366\n",
      "avg: 0.21, eps, 0.010, 0.023, 0\n",
      "run:  367\n",
      "avg: 0.22, eps, 0.010, 0.022, 1\n",
      "run:  368\n",
      "avg: 0.23, eps, 0.010, 0.022, 1\n",
      "run:  369\n",
      "avg: 0.22, eps, 0.010, 0.022, 0\n",
      "run:  370\n",
      "avg: 0.21, eps, 0.010, 0.022, 0\n",
      "run:  371\n",
      "avg: 0.22, eps, 0.010, 0.022, 1\n",
      "run:  372\n",
      "avg: 0.23, eps, 0.010, 0.021, 1\n",
      "run:  373\n",
      "avg: 0.23, eps, 0.010, 0.021, 0\n",
      "run:  374\n",
      "avg: 0.24, eps, 0.010, 0.021, 1\n",
      "run:  375\n",
      "avg: 0.24, eps, 0.010, 0.021, 0\n",
      "run:  376\n",
      "avg: 0.24, eps, 0.010, 0.021, 0\n",
      "run:  377\n",
      "avg: 0.23, eps, 0.010, 0.021, 0\n",
      "run:  378\n",
      "avg: 0.24, eps, 0.010, 0.020, 1\n",
      "run:  379\n",
      "avg: 0.23, eps, 0.010, 0.020, 0\n",
      "run:  380\n",
      "avg: 0.23, eps, 0.010, 0.019, 0\n",
      "run:  381\n",
      "avg: 0.23, eps, 0.010, 0.019, 0\n",
      "run:  382\n",
      "avg: 0.24, eps, 0.010, 0.019, 1\n",
      "run:  383\n",
      "avg: 0.24, eps, 0.010, 0.019, 0\n",
      "run:  384\n",
      "avg: 0.24, eps, 0.010, 0.019, 0\n",
      "run:  385\n",
      "avg: 0.25, eps, 0.010, 0.018, 1\n",
      "run:  386\n",
      "avg: 0.25, eps, 0.010, 0.018, 0\n",
      "run:  387\n",
      "avg: 0.25, eps, 0.010, 0.018, 1\n",
      "run:  388\n",
      "avg: 0.24, eps, 0.010, 0.018, 0\n",
      "run:  389\n",
      "avg: 0.24, eps, 0.010, 0.018, 0\n",
      "run:  390\n",
      "avg: 0.24, eps, 0.010, 0.018, 0\n",
      "run:  391\n",
      "avg: 0.23, eps, 0.010, 0.018, 0\n",
      "run:  392\n",
      "avg: 0.23, eps, 0.010, 0.017, 1\n",
      "run:  393\n",
      "avg: 0.23, eps, 0.010, 0.017, 0\n",
      "run:  394\n",
      "avg: 0.23, eps, 0.010, 0.017, 0\n",
      "run:  395\n",
      "avg: 0.23, eps, 0.010, 0.017, 0\n",
      "run:  396\n",
      "avg: 0.24, eps, 0.010, 0.017, 1\n",
      "run:  397\n",
      "avg: 0.24, eps, 0.010, 0.017, 0\n",
      "run:  398\n",
      "avg: 0.24, eps, 0.010, 0.017, 0\n",
      "run:  399\n",
      "avg: 0.22, eps, 0.010, 0.017, 0\n",
      "run:  400\n",
      "avg: 0.23, eps, 0.010, 0.016, 1\n",
      "run:  401\n",
      "avg: 0.24, eps, 0.010, 0.016, 1\n",
      "run:  402\n",
      "avg: 0.25, eps, 0.010, 0.016, 1\n",
      "run:  403\n",
      "avg: 0.26, eps, 0.010, 0.016, 1\n",
      "run:  404\n",
      "avg: 0.26, eps, 0.010, 0.015, 0\n",
      "run:  405\n",
      "avg: 0.26, eps, 0.010, 0.015, 0\n",
      "run:  406\n",
      "avg: 0.27, eps, 0.010, 0.015, 1\n",
      "run:  407\n",
      "avg: 0.27, eps, 0.010, 0.014, 0\n",
      "run:  408\n",
      "avg: 0.27, eps, 0.010, 0.014, 0\n",
      "run:  409\n",
      "avg: 0.28, eps, 0.010, 0.014, 1\n",
      "run:  410\n",
      "avg: 0.28, eps, 0.010, 0.014, 0\n",
      "run:  411\n",
      "avg: 0.28, eps, 0.010, 0.014, 0\n",
      "run:  412\n",
      "avg: 0.30, eps, 0.010, 0.013, 2\n",
      "run:  413\n",
      "avg: 0.30, eps, 0.010, 0.013, 0\n",
      "run:  414\n",
      "avg: 0.30, eps, 0.010, 0.013, 0\n",
      "run:  415\n",
      "avg: 0.31, eps, 0.010, 0.012, 1\n",
      "run:  416\n",
      "avg: 0.31, eps, 0.010, 0.012, 0\n",
      "run:  417\n",
      "avg: 0.32, eps, 0.010, 0.012, 1\n",
      "run:  418\n",
      "avg: 0.32, eps, 0.010, 0.012, 0\n",
      "run:  419\n",
      "avg: 0.33, eps, 0.010, 0.012, 1\n",
      "run:  420\n",
      "avg: 0.33, eps, 0.010, 0.012, 0\n",
      "run:  421\n",
      "avg: 0.34, eps, 0.010, 0.012, 1\n",
      "run:  422\n",
      "avg: 0.35, eps, 0.010, 0.012, 1\n",
      "run:  423\n",
      "avg: 0.35, eps, 0.010, 0.012, 0\n",
      "run:  424\n",
      "avg: 0.35, eps, 0.010, 0.011, 0\n",
      "run:  425\n",
      "avg: 0.36, eps, 0.010, 0.011, 1\n",
      "run:  426\n",
      "avg: 0.35, eps, 0.010, 0.011, 0\n",
      "run:  427\n",
      "avg: 0.33, eps, 0.010, 0.011, 0\n",
      "run:  428\n",
      "avg: 0.33, eps, 0.010, 0.011, 0\n",
      "run:  429\n",
      "avg: 0.33, eps, 0.010, 0.010, 1\n",
      "run:  430\n",
      "avg: 0.33, eps, 0.010, 0.010, 0\n",
      "run:  431\n",
      "avg: 0.33, eps, 0.010, 0.010, 0\n",
      "run:  432\n",
      "avg: 0.34, eps, 0.010, 0.010, 1\n",
      "run:  433\n",
      "avg: 0.34, eps, 0.010, 0.010, 0\n",
      "run:  434\n",
      "avg: 0.34, eps, 0.010, 0.010, 0\n",
      "run:  435\n",
      "avg: 0.35, eps, 0.010, 0.010, 2\n",
      "run:  436\n",
      "avg: 0.36, eps, 0.010, 0.010, 2\n",
      "run:  437\n",
      "avg: 0.36, eps, 0.010, 0.010, 0\n",
      "run:  438\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  439\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  440\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  441\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  442\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  443\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  444\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  445\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  446\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  447\n",
      "avg: 0.35, eps, 0.010, 0.010, 0\n",
      "run:  448\n",
      "avg: 0.34, eps, 0.010, 0.010, 0\n",
      "run:  449\n",
      "avg: 0.34, eps, 0.010, 0.010, 0\n",
      "run:  450\n",
      "avg: 0.34, eps, 0.010, 0.010, 0\n",
      "run:  451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m observation2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(framestack))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m game\u001b[38;5;241m.\u001b[39mball_speed[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     action1 \u001b[38;5;241m=\u001b[39m agent1\u001b[38;5;241m.\u001b[39mget_action(observation1)\n\u001b[0;32m     52\u001b[0m     reward1, done1, score1, new_state1 \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mplay_step((\u001b[38;5;241m1\u001b[39m, action1))\n\u001b[0;32m     53\u001b[0m     framestack\u001b[38;5;241m.\u001b[39mappend(new_state1)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m---> 38\u001b[0m         state \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     39\u001b[0m         actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_model\u001b[38;5;241m.\u001b[39mforward(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     40\u001b[0m         action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(actions)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    game = pong_ai_game()\n",
    "    state_layers = 1\n",
    "    gamma = 0.9995\n",
    "    epsilon = 1\n",
    "    lr = 0.001\n",
    "    batch_size = 128\n",
    "    action_space =[0,1,2]\n",
    "    training = True\n",
    "    agent1 = Agent(lr, gamma, epsilon, batch_size, 0, action_space)\n",
    "    agent2 = Agent(lr, gamma, epsilon, batch_size, 1, action_space)\n",
    "    framestack = deque(maxlen = 4)\n",
    "    print(agent1.short_model.device)\n",
    "    if training:\n",
    "\n",
    "        scores, eps_hist = [],[]\n",
    "        n_games = 4000\n",
    "\n",
    "        for i in range(n_games):\n",
    "            score = 0\n",
    "            total_reward1, total_reward2 = 0,0\n",
    "            score1, score2 = 0,0\n",
    "            done1, done2 = False, False\n",
    "            game.reset()\n",
    "            print(\"run: \",i)\n",
    "            if i % 250 == 0 and (i != 0 and i != 250):\n",
    "                agent1.epsilon = 0.5\n",
    "                agent2.epsilon = 0.5\n",
    "                agent1.short_model.save('agent1.pth')\n",
    "                agent2.short_model.save('agent2.pth')\n",
    "            observation1 = observation2 = game.get_state()\n",
    "            if i % 20 == 0:\n",
    "                agent1.long_model.load_state_dict(agent1.short_model.state_dict())\n",
    "                agent2.long_model.load_state_dict(agent2.short_model.state_dict())\n",
    "\n",
    "            while not (done1 or done2):\n",
    "                if len(framestack) == 0:\n",
    "                      framestack.append(observation1)\n",
    "                      framestack.append(observation1)\n",
    "                      framestack.append(observation1)\n",
    "                      framestack.append(observation1)\n",
    "\n",
    "                observation1 = torch.stack(list(framestack)).squeeze(1)\n",
    "                observation2 = torch.stack(list(framestack)).squeeze(1)\n",
    "\n",
    "                if game.ball_speed[0] < 0:\n",
    "                    action1 = agent1.get_action(observation1)\n",
    "\n",
    "\n",
    "                    reward1, done1, score1, new_state1 = game.play_step((1, action1))\n",
    "                    framestack.append(new_state1)\n",
    "\n",
    "                    new_state1 = torch.stack(list(framestack)).squeeze(1)\n",
    "\n",
    "                    score = max(score1, score2)\n",
    "                    agent1.store_transition(observation1, action1, reward1[0], new_state1, done1)\n",
    "                    agent1.learn()\n",
    "\n",
    "\n",
    "                    observation1 = new_state1\n",
    "\n",
    "\n",
    "                else:\n",
    "                    action2 = agent2.get_action(observation2)\n",
    "\n",
    "\n",
    "                    reward2, done2, score2, new_state2 = game.play_step((2, action2))\n",
    "                    framestack.append(new_state2)\n",
    "                    new_state2 = torch.stack(list(framestack)).squeeze(1)\n",
    "\n",
    "                    score = max(score1, score2)\n",
    "                    agent2.store_transition(observation2, action2, reward2[1], new_state2, done2)\n",
    "                    agent2.learn()\n",
    "\n",
    "\n",
    "                    observation2 = new_state2\n",
    "\n",
    "            # if i%20 == 0:\n",
    "            #     to_pil = transforms.ToPILImage()\n",
    "            #     for batch in range(min(agent1.state_memory.shape[0], agent1.mem_counter)):\n",
    "            #         img_tensor = agent1.state_memory[batch, 1, :, :].squeeze(0)\n",
    "\n",
    "            #         # Convert to PIL Image\n",
    "            #         img = to_pil(img_tensor)\n",
    "                    \n",
    "            #         # Save image\n",
    "            #         img.save(os.path.join(\"images\", f'image_{i}-{batch}.png'))\n",
    "\n",
    "            scores.append(score)\n",
    "            eps_hist.append(agent1.epsilon)\n",
    "            avg_scores = np.mean(scores[-100:])\n",
    "            print(f\"avg: {avg_scores:0.2f}, eps, {agent1.epsilon:0.3f}, {agent2.epsilon:0.3f}, {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "array = torch.rand((100, 4, 40,30), dtype = torch.float32)\n",
    "print(array.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luise\\AppData\\Local\\Temp\\ipykernel_11116\\1740096457.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(r'model/agent1.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 6.6953e-02,  1.6003e-01,  1.0531e-02],\n",
      "          [ 2.8959e-02,  6.2716e-02,  4.1236e-02],\n",
      "          [ 8.9511e-02,  1.3526e-01,  2.7370e-02]],\n",
      "\n",
      "         [[-1.6957e-01,  4.2939e-02, -9.2830e-02],\n",
      "          [-5.6206e-02, -1.2426e-01,  1.2467e-01],\n",
      "          [-3.7077e-03,  1.0677e-01, -8.6597e-02]],\n",
      "\n",
      "         [[ 1.5719e-01,  1.5460e-01,  7.8460e-02],\n",
      "          [ 5.0477e-02,  9.5723e-02, -2.4567e-02],\n",
      "          [ 1.4559e-01, -2.1400e-02, -1.4344e-01]],\n",
      "\n",
      "         [[-4.3955e-02,  1.4602e-01,  9.3654e-03],\n",
      "          [ 1.1913e-01, -6.1824e-02, -5.0796e-02],\n",
      "          [ 9.5876e-02, -1.3421e-01, -3.2174e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3485e-01, -8.0284e-02, -1.3901e-01],\n",
      "          [-1.6968e-01,  6.2023e-02, -2.2870e-02],\n",
      "          [ 1.8313e-02, -1.2617e-01, -8.2946e-02]],\n",
      "\n",
      "         [[-1.0370e-01, -6.9788e-02, -1.2310e-01],\n",
      "          [-4.7219e-03, -1.9452e-02,  4.8692e-02],\n",
      "          [-1.4473e-01, -5.8029e-02, -9.8315e-02]],\n",
      "\n",
      "         [[ 1.5708e-01,  3.4339e-02, -1.1497e-02],\n",
      "          [-8.4957e-02, -1.0784e-01, -1.1930e-01],\n",
      "          [ 5.8682e-04,  1.0254e-01,  1.4831e-01]],\n",
      "\n",
      "         [[-9.2893e-02,  1.6679e-02, -5.5649e-02],\n",
      "          [ 1.4924e-01,  6.3371e-02,  1.0095e-01],\n",
      "          [ 2.0183e-02,  6.6098e-02,  4.9443e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4220e-01,  1.0807e-01,  1.3444e-01],\n",
      "          [ 1.3350e-01, -1.7026e-01,  1.0998e-03],\n",
      "          [ 1.3007e-04,  4.8172e-02,  7.7011e-02]],\n",
      "\n",
      "         [[ 3.8179e-02, -3.2612e-02,  5.4209e-02],\n",
      "          [-1.7005e-03,  1.4679e-01,  3.8619e-02],\n",
      "          [ 8.1490e-02, -9.7796e-02, -4.6437e-02]],\n",
      "\n",
      "         [[-1.2587e-02, -7.7502e-02,  5.7747e-02],\n",
      "          [ 1.1924e-02,  1.1579e-01, -7.5619e-02],\n",
      "          [-1.1012e-02, -1.6798e-01,  1.5445e-01]],\n",
      "\n",
      "         [[ 7.4820e-02,  1.5233e-01, -1.4323e-01],\n",
      "          [-9.4010e-02, -1.6704e-01, -3.1703e-02],\n",
      "          [-1.2443e-01,  1.0379e-01,  2.3850e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.0021e-02, -1.5174e-01, -3.1804e-02],\n",
      "          [ 7.6586e-02,  1.3584e-01, -4.0599e-02],\n",
      "          [-1.1465e-01, -7.2866e-02, -1.3860e-01]],\n",
      "\n",
      "         [[ 5.1607e-02, -4.9880e-02, -1.6633e-01],\n",
      "          [ 2.4917e-02, -1.3784e-03, -7.1541e-02],\n",
      "          [ 4.2519e-02,  4.4847e-03,  1.1031e-01]],\n",
      "\n",
      "         [[-1.3983e-01, -8.7601e-02,  1.0074e-02],\n",
      "          [-1.5001e-01,  1.4435e-01, -5.3014e-02],\n",
      "          [ 4.3924e-03,  1.2726e-01,  1.5473e-01]],\n",
      "\n",
      "         [[-1.5109e-01, -1.4266e-01,  3.7806e-03],\n",
      "          [ 7.7451e-02, -1.3742e-03,  1.5987e-01],\n",
      "          [ 9.9889e-02, -7.0493e-02, -5.9946e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5874e-01, -8.8225e-03,  1.5787e-01],\n",
      "          [-2.2324e-02, -6.0060e-02,  9.9893e-02],\n",
      "          [ 9.8400e-02, -9.7018e-02,  9.5610e-02]],\n",
      "\n",
      "         [[ 9.0216e-03, -5.9958e-02, -9.9688e-02],\n",
      "          [-1.4438e-01, -5.8704e-02, -1.3000e-01],\n",
      "          [ 1.2620e-01, -1.5492e-01,  1.1144e-02]],\n",
      "\n",
      "         [[ 7.5422e-02,  9.6455e-02,  3.3161e-02],\n",
      "          [-1.5240e-01, -5.7938e-02, -1.3198e-01],\n",
      "          [ 8.4980e-02,  1.0723e-01,  1.5020e-01]],\n",
      "\n",
      "         [[-1.2025e-01, -2.9472e-03, -7.5819e-02],\n",
      "          [-8.7455e-02,  1.1172e-01, -8.7226e-02],\n",
      "          [-1.2579e-02, -1.2204e-01,  7.6589e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2713e-03,  5.6739e-02, -1.4414e-01],\n",
      "          [ 9.7168e-02, -7.2375e-02,  7.7120e-02],\n",
      "          [ 6.9967e-02, -3.9362e-02,  2.8720e-02]],\n",
      "\n",
      "         [[ 1.0984e-01, -9.7546e-02, -8.1393e-02],\n",
      "          [-3.9274e-02, -4.0619e-02, -1.2199e-01],\n",
      "          [-1.4658e-02, -1.2108e-01,  4.9856e-03]],\n",
      "\n",
      "         [[ 8.8326e-02, -7.3734e-03, -6.4907e-03],\n",
      "          [ 1.1815e-01,  1.3217e-01, -2.3729e-02],\n",
      "          [ 2.1037e-02, -1.4273e-01, -8.3179e-03]],\n",
      "\n",
      "         [[-5.2056e-03,  1.2991e-01, -4.0971e-02],\n",
      "          [ 8.1073e-02,  1.1783e-01,  3.1399e-02],\n",
      "          [-1.1839e-01, -7.4624e-03, -9.9497e-02]]]], device='cuda:0')), ('conv1.bias', tensor([ 0.0924,  0.0482,  0.0543,  0.1092, -0.1359, -0.1554,  0.1061, -0.1718,\n",
      "         0.1015, -0.1258,  0.0784,  0.1548, -0.1416, -0.1393, -0.0677, -0.0758,\n",
      "         0.1043, -0.1076,  0.1413,  0.0552, -0.0631,  0.0835, -0.1272, -0.0943,\n",
      "         0.1571,  0.0033, -0.1030,  0.1017,  0.1305,  0.0769,  0.1686, -0.1146],\n",
      "       device='cuda:0')), ('conv2.weight', tensor([[[[ 0.0500, -0.0250, -0.0520],\n",
      "          [-0.0172, -0.0039,  0.0335],\n",
      "          [ 0.0020, -0.0009, -0.0297]],\n",
      "\n",
      "         [[-0.0489, -0.0600, -0.0583],\n",
      "          [-0.0428, -0.0150, -0.0150],\n",
      "          [-0.0365,  0.0103,  0.0424]],\n",
      "\n",
      "         [[ 0.0384, -0.0310, -0.0136],\n",
      "          [ 0.0513,  0.0020,  0.0005],\n",
      "          [-0.0215, -0.0442,  0.0257]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0268,  0.0423,  0.0059],\n",
      "          [ 0.0251, -0.0480, -0.0538],\n",
      "          [ 0.0386, -0.0161,  0.0397]],\n",
      "\n",
      "         [[-0.0308,  0.0403, -0.0193],\n",
      "          [-0.0041, -0.0052, -0.0550],\n",
      "          [-0.0255, -0.0518, -0.0432]],\n",
      "\n",
      "         [[ 0.0426, -0.0270, -0.0386],\n",
      "          [ 0.0027, -0.0570, -0.0340],\n",
      "          [ 0.0013,  0.0456,  0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0255,  0.0200, -0.0432],\n",
      "          [-0.0351, -0.0540,  0.0101],\n",
      "          [-0.0050, -0.0501,  0.0134]],\n",
      "\n",
      "         [[-0.0301, -0.0200, -0.0062],\n",
      "          [-0.0204, -0.0019, -0.0454],\n",
      "          [-0.0087, -0.0381,  0.0400]],\n",
      "\n",
      "         [[-0.0149, -0.0044, -0.0327],\n",
      "          [ 0.0358, -0.0582, -0.0071],\n",
      "          [ 0.0135,  0.0473, -0.0618]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0366,  0.0401, -0.0430],\n",
      "          [-0.0525, -0.0526,  0.0197],\n",
      "          [ 0.0129,  0.0087,  0.0123]],\n",
      "\n",
      "         [[ 0.0168,  0.0352,  0.0299],\n",
      "          [-0.0561,  0.0419,  0.0296],\n",
      "          [ 0.0086, -0.0057,  0.0217]],\n",
      "\n",
      "         [[-0.0342, -0.0227, -0.0444],\n",
      "          [ 0.0210,  0.0034, -0.0166],\n",
      "          [ 0.0026,  0.0095, -0.0306]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0440,  0.0249,  0.0232],\n",
      "          [ 0.0316, -0.0042, -0.0016],\n",
      "          [ 0.0504,  0.0209,  0.0320]],\n",
      "\n",
      "         [[-0.0353,  0.0487, -0.0022],\n",
      "          [-0.0634, -0.0192,  0.0375],\n",
      "          [-0.0471,  0.0396, -0.0227]],\n",
      "\n",
      "         [[ 0.0471,  0.0543,  0.0074],\n",
      "          [ 0.0073, -0.0233, -0.0423],\n",
      "          [-0.0085,  0.0032, -0.0140]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0388, -0.0098, -0.0254],\n",
      "          [ 0.0347, -0.0234, -0.0065],\n",
      "          [ 0.0154,  0.0076,  0.0194]],\n",
      "\n",
      "         [[-0.0333,  0.0497, -0.0047],\n",
      "          [-0.0168, -0.0624,  0.0480],\n",
      "          [-0.0077, -0.0477, -0.0209]],\n",
      "\n",
      "         [[-0.0551, -0.0125, -0.0458],\n",
      "          [-0.0392,  0.0105, -0.0475],\n",
      "          [-0.0283,  0.0308,  0.0323]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0257,  0.0395, -0.0327],\n",
      "          [ 0.0023, -0.0256, -0.0303],\n",
      "          [ 0.0247, -0.0345, -0.0556]],\n",
      "\n",
      "         [[-0.0327, -0.0155,  0.0439],\n",
      "          [-0.0340,  0.0361, -0.0515],\n",
      "          [ 0.0406,  0.0443,  0.0216]],\n",
      "\n",
      "         [[-0.0442, -0.0017, -0.0368],\n",
      "          [-0.0133, -0.0130,  0.0081],\n",
      "          [ 0.0228, -0.0098, -0.0350]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0217, -0.0536, -0.0407],\n",
      "          [-0.0575,  0.0409, -0.0214],\n",
      "          [-0.0407,  0.0394, -0.0350]],\n",
      "\n",
      "         [[ 0.0182,  0.0279,  0.0525],\n",
      "          [ 0.0200,  0.0388,  0.0292],\n",
      "          [-0.0387,  0.0529,  0.0205]],\n",
      "\n",
      "         [[ 0.0174,  0.0292,  0.0153],\n",
      "          [-0.0518,  0.0278,  0.0493],\n",
      "          [ 0.0407, -0.0237, -0.0237]]],\n",
      "\n",
      "\n",
      "        [[[-0.0190, -0.0102,  0.0160],\n",
      "          [ 0.0376,  0.0458,  0.0294],\n",
      "          [ 0.0166, -0.0067, -0.0182]],\n",
      "\n",
      "         [[ 0.0496, -0.0142, -0.0012],\n",
      "          [-0.0507, -0.0355,  0.0066],\n",
      "          [ 0.0490, -0.0451,  0.0502]],\n",
      "\n",
      "         [[ 0.0075, -0.0428,  0.0224],\n",
      "          [ 0.0024,  0.0179, -0.0146],\n",
      "          [ 0.0212, -0.0005,  0.0107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0041, -0.0114,  0.0075],\n",
      "          [ 0.0500,  0.0066, -0.0174],\n",
      "          [ 0.0148,  0.0562, -0.0293]],\n",
      "\n",
      "         [[ 0.0337, -0.0381, -0.0581],\n",
      "          [-0.0385,  0.0176,  0.0215],\n",
      "          [ 0.0283,  0.0216, -0.0289]],\n",
      "\n",
      "         [[-0.0643,  0.0024,  0.0214],\n",
      "          [ 0.0126, -0.0358,  0.0416],\n",
      "          [-0.0129, -0.0431,  0.0420]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0223, -0.0632, -0.0391],\n",
      "          [-0.0066, -0.0519, -0.0372],\n",
      "          [ 0.0385, -0.0451, -0.0142]],\n",
      "\n",
      "         [[-0.0230, -0.0264, -0.0581],\n",
      "          [-0.0377,  0.0046,  0.0144],\n",
      "          [ 0.0181,  0.0504,  0.0048]],\n",
      "\n",
      "         [[-0.0487, -0.0223,  0.0111],\n",
      "          [-0.0200,  0.0500, -0.0099],\n",
      "          [ 0.0441, -0.0234, -0.0220]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0568, -0.0042,  0.0157],\n",
      "          [-0.0559,  0.0140, -0.0080],\n",
      "          [ 0.0542, -0.0130,  0.0393]],\n",
      "\n",
      "         [[-0.0143, -0.0078, -0.0374],\n",
      "          [-0.0454, -0.0321, -0.0188],\n",
      "          [ 0.0318, -0.0085,  0.0525]],\n",
      "\n",
      "         [[-0.0509, -0.0281, -0.0092],\n",
      "          [ 0.0076, -0.0136, -0.0561],\n",
      "          [-0.0047, -0.0033, -0.0318]]]], device='cuda:0')), ('conv2.bias', tensor([-0.0211,  0.0060, -0.0612,  0.0289, -0.0352,  0.0440,  0.0013, -0.0070,\n",
      "        -0.0294, -0.0209, -0.0456, -0.0224, -0.0446, -0.0428,  0.0297, -0.0339,\n",
      "         0.0005, -0.0294, -0.0248,  0.0482, -0.0246, -0.0240,  0.0074, -0.0046,\n",
      "        -0.0268,  0.0018,  0.0043, -0.0529, -0.0018,  0.0346, -0.0027, -0.0338,\n",
      "        -0.0394,  0.0186,  0.0411,  0.0082, -0.0251, -0.0056,  0.0089,  0.0441,\n",
      "         0.0391, -0.0242,  0.0620,  0.0362,  0.0350,  0.0111, -0.0622, -0.0363,\n",
      "        -0.0490, -0.0126,  0.0219, -0.0122,  0.0005, -0.0231, -0.0540, -0.0392,\n",
      "         0.0180,  0.0044, -0.0220, -0.0575, -0.0555, -0.0299, -0.0097,  0.0295],\n",
      "       device='cuda:0')), ('conv3.weight', tensor([[[[ 2.8368e-02, -1.5622e-02, -1.7439e-02],\n",
      "          [ 2.2380e-02,  1.8696e-03,  3.8640e-02],\n",
      "          [ 2.0750e-02, -1.2965e-03,  7.7649e-03]],\n",
      "\n",
      "         [[ 3.1646e-02,  1.7142e-02, -3.6675e-02],\n",
      "          [-2.1652e-02,  3.2511e-02, -3.1671e-02],\n",
      "          [-2.7179e-02, -5.6830e-03, -1.2838e-02]],\n",
      "\n",
      "         [[ 1.6774e-02, -3.7754e-03, -2.0133e-02],\n",
      "          [ 2.1839e-02, -1.1176e-02, -2.3348e-02],\n",
      "          [-4.2822e-02, -2.7740e-03,  5.0508e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6852e-02, -2.1500e-02, -1.9883e-02],\n",
      "          [-1.2088e-02,  4.0384e-02, -2.3621e-02],\n",
      "          [ 9.3686e-03, -3.1573e-02,  4.0956e-02]],\n",
      "\n",
      "         [[-3.7029e-03, -4.4694e-02,  4.4473e-02],\n",
      "          [-2.8291e-02,  2.0049e-02,  4.4512e-02],\n",
      "          [-1.5842e-02, -3.2569e-02, -1.0753e-02]],\n",
      "\n",
      "         [[ 1.9458e-02, -8.9930e-03, -3.2001e-02],\n",
      "          [-3.2294e-02,  5.2265e-03, -4.8762e-03],\n",
      "          [ 5.3909e-03, -1.9361e-02,  3.6306e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8363e-02, -2.0647e-02, -1.2612e-02],\n",
      "          [ 1.9867e-02,  1.1979e-02, -3.7166e-02],\n",
      "          [ 5.4787e-03,  9.3981e-03,  8.5314e-03]],\n",
      "\n",
      "         [[ 2.0800e-02,  1.5564e-02,  4.8166e-03],\n",
      "          [-2.7317e-03,  9.1323e-03, -3.1513e-02],\n",
      "          [-3.1738e-02,  2.6423e-02, -3.9402e-02]],\n",
      "\n",
      "         [[-3.5213e-02, -2.4109e-02,  8.0670e-03],\n",
      "          [ 7.7624e-03, -8.5160e-03, -4.0124e-02],\n",
      "          [ 1.0339e-03,  3.3502e-03,  2.0099e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6171e-02, -7.8544e-03, -3.9159e-02],\n",
      "          [-4.3707e-02,  3.1011e-02, -4.2402e-02],\n",
      "          [ 3.0011e-02, -1.0511e-02, -2.9378e-02]],\n",
      "\n",
      "         [[-1.1124e-02,  1.9663e-02, -1.4655e-02],\n",
      "          [ 2.2961e-02, -1.7726e-03, -7.4051e-03],\n",
      "          [-9.0678e-04,  3.4533e-02, -8.7665e-03]],\n",
      "\n",
      "         [[-2.0261e-02, -3.4727e-02, -4.5896e-03],\n",
      "          [-2.8156e-02,  1.9342e-02, -2.4088e-02],\n",
      "          [ 5.3005e-03,  1.2489e-02, -1.8632e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4266e-03, -2.6468e-02,  3.7784e-03],\n",
      "          [-4.3212e-02,  3.3342e-02,  4.3259e-03],\n",
      "          [ 1.0326e-02,  2.6222e-02, -3.1197e-02]],\n",
      "\n",
      "         [[ 1.9538e-02,  3.4720e-02, -4.0090e-02],\n",
      "          [-4.3165e-02,  1.7869e-02,  1.2054e-02],\n",
      "          [ 3.7083e-02, -2.3474e-02, -4.0188e-03]],\n",
      "\n",
      "         [[-3.2380e-02, -7.0773e-03, -7.1458e-03],\n",
      "          [ 1.3001e-02,  2.7479e-02,  1.9830e-02],\n",
      "          [-3.7486e-02, -3.3050e-02,  3.1606e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4450e-02, -2.2084e-02,  9.0359e-03],\n",
      "          [ 2.3651e-02,  6.3707e-03, -3.7730e-02],\n",
      "          [-6.3535e-03,  1.9115e-02, -4.0376e-02]],\n",
      "\n",
      "         [[-3.7880e-02,  8.4334e-03, -2.2168e-02],\n",
      "          [ 1.8126e-02,  1.3383e-03, -7.1701e-03],\n",
      "          [-5.5700e-05, -3.4994e-02, -1.4979e-02]],\n",
      "\n",
      "         [[ 2.4199e-02, -4.3880e-03, -3.2870e-02],\n",
      "          [-1.0435e-03,  1.7149e-02, -1.3983e-02],\n",
      "          [ 2.4149e-02, -1.2654e-02, -4.6724e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.8164e-02, -2.4258e-02,  3.2555e-02],\n",
      "          [-1.7102e-02,  1.4351e-02,  1.7685e-02],\n",
      "          [ 3.2700e-02, -4.2375e-02, -3.2782e-02]],\n",
      "\n",
      "         [[ 3.1898e-02, -1.8924e-02,  3.2976e-02],\n",
      "          [-4.0023e-02, -2.0000e-02, -1.8776e-02],\n",
      "          [-3.8959e-02,  8.1612e-03,  9.0942e-03]],\n",
      "\n",
      "         [[ 3.0774e-02,  1.0541e-02,  1.1087e-02],\n",
      "          [ 2.3090e-02,  5.2420e-03, -2.3208e-02],\n",
      "          [-2.4078e-03,  1.9176e-02,  2.1886e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2668e-02,  4.1314e-03, -1.2576e-02],\n",
      "          [ 2.6397e-02,  1.1056e-02,  8.5812e-03],\n",
      "          [ 2.9041e-02, -2.3032e-02, -4.5659e-02]],\n",
      "\n",
      "         [[ 2.6952e-02,  3.0949e-02,  2.6141e-02],\n",
      "          [ 8.1582e-03,  3.9280e-02,  1.3986e-02],\n",
      "          [-1.5526e-02,  2.1240e-02, -1.3456e-02]],\n",
      "\n",
      "         [[-1.9562e-02, -4.5151e-03,  7.3672e-03],\n",
      "          [ 2.0924e-02,  2.8429e-02, -8.6267e-03],\n",
      "          [-3.0220e-02,  1.7962e-02,  1.6493e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1782e-03, -2.3032e-02,  2.3189e-02],\n",
      "          [-6.4791e-03,  2.5324e-03,  2.4752e-02],\n",
      "          [-3.4339e-02, -1.3585e-02,  2.2371e-02]],\n",
      "\n",
      "         [[-1.6902e-02, -2.9594e-02, -7.5053e-03],\n",
      "          [ 5.0772e-03,  1.8373e-02,  2.2038e-03],\n",
      "          [-2.1787e-02, -3.8962e-02, -2.4776e-02]],\n",
      "\n",
      "         [[ 7.4291e-03,  1.0889e-02,  2.1769e-02],\n",
      "          [ 3.3446e-02,  5.6032e-03,  2.5561e-02],\n",
      "          [-3.8638e-02, -1.9270e-02, -3.2292e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5671e-02, -2.6463e-02, -2.7131e-02],\n",
      "          [-1.2319e-02, -5.8427e-03, -2.2182e-02],\n",
      "          [-2.3322e-02,  9.7415e-04, -1.6256e-02]],\n",
      "\n",
      "         [[-4.0066e-02, -4.9704e-03, -3.6435e-02],\n",
      "          [ 2.2415e-02, -5.9589e-03,  3.0340e-02],\n",
      "          [-5.6606e-03,  1.5337e-02, -4.1949e-02]],\n",
      "\n",
      "         [[-2.1177e-02, -1.9028e-03,  1.4703e-02],\n",
      "          [-2.3682e-03, -4.1964e-02, -2.4961e-02],\n",
      "          [-2.4091e-02,  2.9584e-02,  2.5669e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0686e-02,  1.7280e-02,  2.2995e-03],\n",
      "          [-3.1546e-02, -1.3205e-03,  2.2703e-02],\n",
      "          [-3.5511e-02,  1.2162e-02, -1.3680e-02]],\n",
      "\n",
      "         [[-8.7090e-03, -7.1811e-03,  1.7751e-02],\n",
      "          [-3.1409e-02,  6.6065e-03, -2.6872e-02],\n",
      "          [ 3.0515e-02, -1.6484e-02,  1.9605e-02]],\n",
      "\n",
      "         [[ 3.6720e-02,  1.5688e-02,  1.3710e-03],\n",
      "          [-1.0132e-02, -1.7685e-02, -3.0799e-02],\n",
      "          [-4.5345e-03,  2.9629e-04,  1.7358e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3113e-02, -3.8591e-02, -1.9380e-02],\n",
      "          [ 3.1849e-02, -4.6251e-02, -3.8267e-02],\n",
      "          [-3.3900e-02,  2.2719e-04,  1.0205e-02]],\n",
      "\n",
      "         [[ 2.8690e-02,  1.6997e-02,  1.5446e-02],\n",
      "          [ 3.6275e-02, -2.4504e-02, -2.3992e-02],\n",
      "          [-2.1713e-02, -4.6104e-02,  6.8345e-03]],\n",
      "\n",
      "         [[-1.4330e-02, -3.9334e-02, -1.0571e-02],\n",
      "          [ 2.9897e-02,  2.4911e-02, -4.3587e-02],\n",
      "          [-7.4674e-03, -2.7431e-02, -1.1439e-02]]]], device='cuda:0')), ('conv3.bias', tensor([ 0.0224, -0.0019, -0.0348,  0.0143, -0.0286,  0.0024,  0.0012,  0.0036,\n",
      "        -0.0191,  0.0092,  0.0289,  0.0022,  0.0247, -0.0393,  0.0085, -0.0337,\n",
      "         0.0132,  0.0261,  0.0321, -0.0333,  0.0009,  0.0026, -0.0432,  0.0103,\n",
      "         0.0014, -0.0293, -0.0174,  0.0091, -0.0341,  0.0148, -0.0226,  0.0030,\n",
      "         0.0020, -0.0067,  0.0086, -0.0332, -0.0351,  0.0219,  0.0063,  0.0010,\n",
      "        -0.0477,  0.0043,  0.0171, -0.0338,  0.0240,  0.0280,  0.0280, -0.0255,\n",
      "         0.0037, -0.0406, -0.0113, -0.0152, -0.0107, -0.0056,  0.0054, -0.0113,\n",
      "        -0.0102, -0.0041, -0.0355, -0.0293,  0.0086, -0.0228,  0.0032,  0.0273],\n",
      "       device='cuda:0')), ('linear1.weight', tensor([[ 1.2062e-03,  1.6425e-03, -1.5974e-03,  ..., -7.7016e-03,\n",
      "         -3.9386e-03, -6.3535e-03],\n",
      "        [ 9.8545e-03,  1.6357e-03,  9.8396e-04,  ..., -4.0121e-04,\n",
      "         -2.4181e-03, -1.2602e-03],\n",
      "        [ 2.1604e-03, -1.2011e-03, -6.7589e-04,  ...,  2.1880e-03,\n",
      "          5.0141e-03,  7.2579e-03],\n",
      "        ...,\n",
      "        [ 3.4278e-03,  4.9888e-03,  7.6891e-03,  ..., -7.0494e-03,\n",
      "         -7.6882e-03, -5.6402e-03],\n",
      "        [-7.1886e-04, -2.0280e-03, -4.0525e-03,  ..., -2.6970e-03,\n",
      "         -4.6610e-03, -2.7308e-03],\n",
      "        [ 8.3294e-03,  8.1622e-03,  4.1358e-03,  ..., -7.8259e-03,\n",
      "         -2.9282e-03,  3.6467e-05]], device='cuda:0')), ('linear1.bias', tensor([-0.0046, -0.0023, -0.0049, -0.0073, -0.0023, -0.0056, -0.0074, -0.0030,\n",
      "        -0.0056, -0.0024, -0.0082, -0.0015, -0.0014, -0.0099, -0.0011, -0.0052,\n",
      "        -0.0078, -0.0033, -0.0106, -0.0080, -0.0022, -0.0062, -0.0048, -0.0038,\n",
      "        -0.0034, -0.0081, -0.0127, -0.0051, -0.0031, -0.0130, -0.0052, -0.0053,\n",
      "        -0.0045, -0.0069, -0.0053, -0.0034, -0.0069, -0.0041, -0.0030, -0.0041,\n",
      "        -0.0081, -0.0013, -0.0096, -0.0063, -0.0021, -0.0119, -0.0036, -0.0034,\n",
      "        -0.0039, -0.0048, -0.0103, -0.0043, -0.0030, -0.0016, -0.0068, -0.0059,\n",
      "        -0.0034, -0.0036, -0.0056, -0.0018, -0.0106, -0.0111, -0.0018, -0.0024,\n",
      "        -0.0012, -0.0102, -0.0068, -0.0021, -0.0017, -0.0020, -0.0047, -0.0092,\n",
      "        -0.0067, -0.0015, -0.0112, -0.0118, -0.0067, -0.0062, -0.0034, -0.0040,\n",
      "        -0.0030, -0.0025, -0.0074, -0.0026, -0.0026, -0.0028, -0.0144, -0.0076,\n",
      "        -0.0071, -0.0100, -0.0016, -0.0065, -0.0084, -0.0044, -0.0036, -0.0062,\n",
      "        -0.0027, -0.0048, -0.0064, -0.0145, -0.0013, -0.0027, -0.0083, -0.0068,\n",
      "        -0.0045, -0.0075, -0.0038, -0.0126, -0.0034, -0.0114, -0.0063, -0.0086,\n",
      "        -0.0157, -0.0035, -0.0066, -0.0020, -0.0020, -0.0023, -0.0045, -0.0189,\n",
      "        -0.0065, -0.0036, -0.0019, -0.0125, -0.0262, -0.0029, -0.0016, -0.0016,\n",
      "        -0.0028, -0.0039, -0.0010, -0.0035, -0.0081, -0.0093, -0.0067, -0.0046,\n",
      "        -0.0075, -0.0040, -0.0042, -0.0077, -0.0035, -0.0040, -0.0123, -0.0058,\n",
      "        -0.0015, -0.0015, -0.0104, -0.0027, -0.0010, -0.0077, -0.0054, -0.0077,\n",
      "        -0.0008, -0.0081, -0.0039, -0.0016, -0.0070, -0.0087, -0.0022, -0.0054,\n",
      "        -0.0058, -0.0047, -0.0114, -0.0026, -0.0031, -0.0119, -0.0071, -0.0017,\n",
      "        -0.0037, -0.0050, -0.0022, -0.0034, -0.0037, -0.0107, -0.0105, -0.0024,\n",
      "        -0.0038, -0.0060, -0.0058, -0.0060, -0.0024, -0.0045, -0.0079, -0.0095,\n",
      "        -0.0053, -0.0120, -0.0079, -0.0039, -0.0030, -0.0036, -0.0098, -0.0028,\n",
      "        -0.0073, -0.0102, -0.0089, -0.0015, -0.0046, -0.0049, -0.0044, -0.0049,\n",
      "        -0.0036, -0.0040, -0.0016, -0.0055, -0.0015, -0.0022, -0.0090, -0.0084,\n",
      "        -0.0064, -0.0022, -0.0049, -0.0048, -0.0029, -0.0049, -0.0093, -0.0037,\n",
      "        -0.0023, -0.0105, -0.0204, -0.0151, -0.0113, -0.0050, -0.0088, -0.0032,\n",
      "        -0.0044, -0.0077, -0.0113, -0.0021, -0.0044, -0.0071, -0.0017, -0.0077,\n",
      "        -0.0023, -0.0043, -0.0129, -0.0056, -0.0040, -0.0031, -0.0026, -0.0032,\n",
      "        -0.0045, -0.0081, -0.0085, -0.0178, -0.0111, -0.0019, -0.0022, -0.0041,\n",
      "        -0.0040, -0.0067, -0.0062, -0.0035, -0.0035, -0.0036, -0.0052, -0.0014],\n",
      "       device='cuda:0')), ('linear2.weight', tensor([[-5.2421e-02, -1.5435e-02,  1.0883e-01,  4.4448e-02,  2.4669e-02,\n",
      "          9.1641e-03,  4.3704e-02,  3.5235e-02,  2.6494e-02,  1.0501e-01,\n",
      "          5.3652e-02, -2.6544e-02, -5.3952e-02,  5.3602e-02,  4.8484e-02,\n",
      "          3.5427e-02,  8.4499e-03, -5.2774e-02,  3.8123e-02,  1.5893e-02,\n",
      "         -4.2496e-02, -4.9314e-03,  6.9019e-02, -6.4907e-02,  1.0813e-02,\n",
      "          2.2886e-03,  6.9070e-03, -5.4639e-02, -4.8529e-02, -4.1743e-02,\n",
      "          1.2889e-02,  8.8272e-02, -5.5677e-02, -4.4551e-02, -5.1607e-02,\n",
      "         -5.5446e-02,  3.7399e-02,  3.7617e-02, -2.3467e-02,  1.2116e-02,\n",
      "          3.3452e-02,  4.7102e-02,  2.9387e-02, -1.5920e-02,  1.7517e-02,\n",
      "          3.1522e-02,  1.4355e-01,  3.1985e-02,  2.1345e-02,  3.4114e-03,\n",
      "          4.6009e-02, -9.4466e-03, -4.3282e-03,  1.2888e-02, -4.0203e-02,\n",
      "         -3.8268e-02,  5.2815e-02,  4.0695e-03, -4.5976e-02, -7.6647e-02,\n",
      "         -8.6034e-03, -8.9561e-03,  2.2054e-02, -4.7907e-02,  6.5105e-02,\n",
      "         -4.7079e-02, -3.0410e-02,  3.5076e-02,  4.2499e-02, -3.7063e-02,\n",
      "         -1.7999e-02,  5.8824e-02,  7.2540e-02,  8.3357e-02, -8.0204e-03,\n",
      "         -2.6851e-03,  2.0270e-02, -4.4759e-03,  2.5676e-02,  2.1508e-02,\n",
      "          1.5810e-01, -5.7409e-02, -4.7415e-02,  2.8586e-02,  3.8163e-03,\n",
      "          3.7250e-03,  1.1709e-02, -2.6916e-03, -1.5107e-02,  2.2617e-02,\n",
      "         -2.6526e-02, -3.6101e-02, -3.3653e-02, -3.1207e-02,  2.2311e-02,\n",
      "         -9.6818e-03, -1.4473e-02, -1.0421e-02, -2.1367e-02, -3.4782e-02,\n",
      "          2.8358e-02,  2.9280e-02,  1.3295e-02,  7.6641e-04, -2.0822e-02,\n",
      "         -2.8311e-02, -3.7337e-02,  1.0367e-03,  5.8466e-02, -3.9678e-04,\n",
      "         -5.6182e-02, -4.3207e-02,  2.0993e-02,  5.2077e-02, -3.2870e-02,\n",
      "          5.7161e-02,  4.4339e-02,  4.4431e-02,  4.2410e-02, -6.5545e-02,\n",
      "          8.5802e-03,  4.8864e-02,  5.2118e-02,  4.4563e-02,  1.9102e-04,\n",
      "         -6.8020e-03, -5.8278e-02, -2.9925e-02,  1.8985e-02,  9.4234e-03,\n",
      "         -2.7010e-02, -4.9404e-02,  2.9079e-02, -5.5956e-02,  2.6676e-02,\n",
      "         -5.4103e-02, -1.3662e-02,  1.4554e-02,  3.8275e-02,  1.0689e-01,\n",
      "         -3.6426e-02,  9.4485e-02, -7.1551e-03,  1.7148e-02,  3.2045e-03,\n",
      "          4.5260e-02,  3.7031e-02,  1.8392e-02,  9.5422e-02, -4.2787e-02,\n",
      "         -7.9746e-03, -2.3600e-02,  2.3917e-02, -1.8469e-02,  3.5414e-02,\n",
      "          1.6979e-02,  1.8568e-02,  4.2450e-02, -1.5635e-02, -8.6713e-03,\n",
      "          1.2811e-02,  5.7934e-02,  5.4167e-02, -6.6547e-02, -4.5862e-02,\n",
      "          4.1911e-02, -1.8110e-02, -1.9699e-02, -1.8489e-02,  3.1536e-02,\n",
      "          5.2432e-02, -4.6667e-02,  3.4549e-03,  3.4543e-02,  2.5408e-02,\n",
      "          1.0185e-02,  2.1989e-02, -5.9474e-02,  6.2896e-02, -1.7917e-02,\n",
      "          7.3519e-03, -2.3929e-02,  6.0208e-03, -1.4677e-02,  1.7245e-02,\n",
      "         -2.3486e-02, -1.6089e-02, -1.8216e-02,  1.6204e-03, -3.1161e-02,\n",
      "         -3.4641e-02, -1.1250e-02,  1.3716e-02,  2.7690e-02, -4.8388e-04,\n",
      "          4.2435e-02, -9.9977e-03, -2.8491e-02, -1.8523e-02, -9.3096e-03,\n",
      "          2.3634e-02,  5.4133e-02,  1.2452e-01, -5.2184e-02, -2.0979e-02,\n",
      "          8.9510e-02, -2.4089e-02,  2.2826e-02, -2.1454e-02,  3.2207e-02,\n",
      "          4.6486e-02, -3.9663e-02, -4.5720e-03, -3.8490e-02,  3.0252e-02,\n",
      "          3.1671e-02,  2.1103e-02, -2.0425e-02,  6.5420e-02, -1.5638e-02,\n",
      "         -3.0262e-02,  7.7334e-04,  4.4293e-02, -1.2477e-02, -5.1497e-03,\n",
      "          1.4730e-02,  1.0956e-02, -5.0835e-02, -4.6382e-02,  1.2641e-02,\n",
      "          6.4442e-02,  3.3606e-02,  6.0149e-03, -6.0737e-02, -9.1817e-03,\n",
      "         -5.8621e-02,  1.8349e-02, -2.1232e-02, -3.9224e-02,  1.4488e-01,\n",
      "         -1.9641e-02,  2.0422e-02, -3.8972e-02,  2.7241e-02,  5.8068e-03,\n",
      "          1.3406e-01,  2.3991e-03,  1.1929e-02,  4.7080e-02, -3.9755e-02,\n",
      "         -1.1133e-02, -5.5525e-02,  3.3921e-02,  8.6597e-03,  1.1813e-01,\n",
      "         -5.9868e-02],\n",
      "        [ 4.3150e-02, -4.4735e-02,  4.8217e-02,  1.5027e-02,  1.8001e-02,\n",
      "          3.8123e-02,  4.7516e-03, -2.1804e-02,  2.6453e-02,  9.5847e-02,\n",
      "          2.2157e-02,  2.1010e-02,  6.2120e-02,  6.4471e-02,  3.3340e-03,\n",
      "         -4.4788e-03, -1.3595e-02, -1.8549e-02,  3.5705e-02,  4.7617e-02,\n",
      "          1.3315e-02,  1.6568e-03,  4.5250e-02, -3.4488e-02,  1.3097e-01,\n",
      "         -5.5203e-02, -4.0380e-02,  1.6571e-02,  1.7687e-02,  4.4479e-02,\n",
      "          3.1765e-02,  2.8769e-02,  5.5078e-02,  6.2794e-02, -1.7279e-02,\n",
      "         -3.2022e-02, -2.0448e-02, -2.8693e-02, -1.1438e-02, -6.7980e-04,\n",
      "         -1.4768e-02, -2.8468e-02, -2.7336e-02,  2.0495e-02,  3.7672e-02,\n",
      "          9.4213e-03,  3.2391e-02,  4.1828e-02,  3.9103e-02, -2.2931e-02,\n",
      "          4.2486e-03,  2.0445e-02,  3.9678e-02, -4.0750e-03,  2.1713e-02,\n",
      "          4.0835e-02,  2.3234e-02, -2.6826e-02,  4.5060e-02,  2.5982e-03,\n",
      "          5.1087e-02, -4.2952e-02,  8.3074e-03,  2.6294e-02,  5.3796e-02,\n",
      "         -8.2119e-03,  2.6042e-02, -2.7589e-02, -5.4556e-02,  1.6796e-02,\n",
      "          5.1677e-02,  3.7516e-02, -4.3366e-02,  4.1209e-02,  2.8883e-02,\n",
      "          9.4098e-03,  5.5528e-02, -2.0533e-02,  2.2337e-02,  3.1533e-02,\n",
      "          2.2016e-02,  1.4291e-03,  4.6853e-02,  7.3061e-03,  4.2085e-02,\n",
      "          1.6510e-02, -3.1666e-05, -5.3817e-02, -4.1073e-02, -4.9412e-03,\n",
      "          1.7193e-02, -5.1055e-03,  5.2412e-02, -4.3142e-02,  2.0014e-02,\n",
      "         -6.6871e-03,  8.5137e-03, -1.8004e-02,  1.2225e-02, -4.2380e-03,\n",
      "          1.8318e-02,  3.3833e-02, -4.4629e-02,  4.1144e-02,  1.3339e-02,\n",
      "         -2.6805e-03,  5.8766e-02,  3.2175e-02, -3.6693e-02,  4.4988e-02,\n",
      "         -3.8467e-02, -5.0461e-02, -3.4693e-02,  4.5230e-02,  3.2104e-02,\n",
      "         -2.1647e-02, -2.8910e-02,  4.3812e-03, -1.2048e-02,  2.3841e-02,\n",
      "         -4.3418e-02, -3.4197e-02,  4.0213e-02,  2.8633e-02, -2.5286e-02,\n",
      "         -5.9090e-02,  4.2432e-02, -5.4212e-02,  5.5815e-02, -1.9904e-02,\n",
      "          2.1224e-03, -3.1938e-02,  2.3900e-04,  2.5307e-02, -2.9943e-02,\n",
      "         -5.5742e-02, -4.6318e-02, -2.1579e-02,  1.3323e-02, -1.6649e-02,\n",
      "          4.8484e-02,  3.3467e-02,  5.3546e-02, -2.3507e-02, -1.2526e-03,\n",
      "          4.2499e-02,  4.6460e-02,  4.3742e-02,  7.3363e-02, -3.3521e-02,\n",
      "          5.5846e-02,  2.6765e-02,  4.9939e-02, -3.6461e-02, -4.3979e-02,\n",
      "          5.7802e-02, -9.8855e-03, -1.0204e-03,  1.3156e-02, -4.7582e-02,\n",
      "          5.4518e-02, -4.9481e-02,  5.0091e-02, -2.2402e-02, -1.6044e-02,\n",
      "         -3.8138e-02,  2.5637e-02,  1.5371e-03,  2.5705e-02,  4.6829e-02,\n",
      "          2.1155e-02, -5.5671e-02,  4.2958e-02, -8.1993e-03,  6.5449e-02,\n",
      "         -2.6241e-02,  1.6568e-03,  5.1403e-02,  1.9376e-02, -4.7894e-02,\n",
      "         -5.9564e-02,  3.6945e-02,  5.1956e-02, -5.2936e-02, -4.1071e-02,\n",
      "          1.2831e-02,  7.7337e-03, -3.1981e-03, -5.0873e-02, -1.5888e-02,\n",
      "          3.7466e-02, -3.4374e-02, -4.7543e-02,  3.7157e-02,  4.8077e-02,\n",
      "          4.2654e-02,  4.6952e-02,  6.0977e-02,  2.7892e-02, -7.9527e-03,\n",
      "          3.7890e-02, -3.6561e-02,  2.9207e-02, -1.7729e-02, -1.6245e-03,\n",
      "          4.1564e-02,  4.2366e-02,  9.4304e-03, -2.2182e-02, -1.3960e-02,\n",
      "          3.4038e-02,  1.5455e-02,  2.5748e-02,  1.0963e-02, -4.5924e-02,\n",
      "          9.9817e-04,  1.1797e-02,  6.0089e-02, -4.9408e-02, -9.6064e-03,\n",
      "          3.8240e-02, -4.5489e-02,  3.1181e-02, -1.6559e-02,  5.9855e-02,\n",
      "          4.5954e-02,  2.1897e-02, -4.1517e-02,  3.8162e-03,  4.2554e-03,\n",
      "          4.2462e-02,  5.2091e-02, -1.0923e-02,  1.8918e-02, -1.4801e-02,\n",
      "          4.1114e-02, -6.0594e-04,  9.0293e-05, -2.2644e-02,  2.7772e-02,\n",
      "          6.3765e-02, -3.3832e-02,  4.0767e-02, -2.2338e-02,  3.2946e-02,\n",
      "          2.3254e-02, -4.8575e-02, -7.9235e-03,  2.8029e-02, -3.1899e-02,\n",
      "         -1.9997e-03, -4.1581e-02, -3.8156e-02,  2.0836e-02,  7.7372e-02,\n",
      "         -1.8926e-02],\n",
      "        [ 2.9601e-02, -4.2033e-02,  3.9564e-02, -2.2212e-02, -2.4140e-02,\n",
      "         -2.3292e-02,  2.2920e-02,  2.6201e-03, -3.7119e-02,  6.6129e-02,\n",
      "          5.9491e-03, -5.2135e-02, -2.1557e-02,  4.8226e-02, -4.6582e-02,\n",
      "         -4.3786e-02, -2.9958e-02,  3.2999e-02,  3.0041e-02,  1.1842e-02,\n",
      "         -1.7332e-02,  1.3042e-02,  4.2616e-02, -5.6251e-02,  1.3136e-01,\n",
      "          1.1920e-02,  7.4808e-04,  6.2578e-03, -3.6457e-02,  1.9005e-02,\n",
      "         -4.4427e-03, -1.2892e-02, -6.1433e-03,  2.5835e-02,  2.9383e-02,\n",
      "         -3.8312e-02, -4.8817e-02, -2.8114e-02, -1.3301e-02,  1.2138e-02,\n",
      "         -4.3229e-03,  1.0889e-02,  1.7189e-02,  2.8179e-02,  2.3150e-02,\n",
      "         -2.9809e-02,  4.7191e-02,  3.0085e-02, -3.3425e-02,  4.0458e-02,\n",
      "          4.8874e-02,  4.8086e-02, -1.9051e-02, -5.4014e-02,  3.1190e-03,\n",
      "         -2.3596e-03, -1.6404e-02,  4.2483e-03,  5.4040e-02, -4.8497e-02,\n",
      "          4.5718e-02,  3.7047e-03, -5.5001e-02,  4.0007e-02,  1.1609e-02,\n",
      "          7.5307e-03,  6.2385e-04,  1.4112e-03, -4.6731e-02, -4.3744e-02,\n",
      "          5.4384e-02,  1.6445e-03,  3.7894e-02,  1.6521e-02,  2.3745e-02,\n",
      "         -4.7717e-03, -2.4430e-03,  5.1192e-02,  3.9779e-03, -2.3604e-02,\n",
      "          3.0387e-02, -4.2808e-02, -8.0587e-03,  4.3836e-02, -3.6475e-02,\n",
      "         -5.3538e-02,  4.0030e-02, -4.2563e-02, -2.1228e-02,  2.0186e-02,\n",
      "         -4.4142e-02, -3.5801e-02,  1.0512e-02, -3.3416e-02, -1.6710e-02,\n",
      "          1.8750e-02,  4.7911e-02, -1.9283e-02,  3.3883e-02, -1.7761e-02,\n",
      "         -2.5528e-02,  7.7831e-03, -2.3820e-02, -2.2195e-02,  3.8930e-02,\n",
      "         -9.7027e-03,  5.4788e-03, -1.5142e-02, -6.0081e-02, -2.6083e-02,\n",
      "          9.5770e-03,  1.9198e-02, -6.3243e-03,  4.3929e-02, -4.0656e-02,\n",
      "         -3.0889e-02, -4.9150e-03, -1.8190e-02, -4.8253e-02,  3.5383e-04,\n",
      "          4.5585e-02, -2.4644e-02, -2.2385e-02,  1.8225e-02, -5.3759e-03,\n",
      "         -5.1896e-02, -2.0456e-02,  4.9959e-02,  2.0483e-02,  3.9035e-03,\n",
      "         -9.5148e-03,  1.6428e-02, -2.0258e-03, -1.3246e-02,  1.3707e-02,\n",
      "         -1.4076e-02,  3.0536e-02, -5.8562e-02, -5.7702e-02, -1.1837e-02,\n",
      "          2.0065e-02,  8.5509e-02,  1.1954e-02,  4.0641e-03, -5.6915e-03,\n",
      "          5.6966e-02,  4.2773e-02, -4.5021e-02,  9.8199e-02, -6.9441e-03,\n",
      "          4.5766e-02,  1.1885e-02, -5.3614e-04, -4.4309e-02,  2.1379e-02,\n",
      "          3.8080e-02,  5.3959e-02,  5.5043e-02, -2.2529e-02, -4.2764e-02,\n",
      "         -4.0238e-03,  3.0720e-02, -3.2474e-02, -4.4228e-02,  2.4920e-02,\n",
      "          2.9247e-02,  2.6818e-02, -9.8274e-03, -3.1601e-02, -2.5552e-02,\n",
      "         -6.1892e-02,  3.0380e-02,  4.7257e-02, -2.1479e-03,  2.6464e-02,\n",
      "         -4.8003e-02,  3.1802e-02, -1.7122e-02,  3.0788e-02,  4.2113e-02,\n",
      "         -4.0390e-02,  8.0401e-03, -4.1120e-02, -8.5860e-03,  2.1138e-02,\n",
      "         -1.4687e-02,  9.1190e-03,  2.0454e-02, -4.8511e-02,  2.4071e-02,\n",
      "          4.5840e-02,  1.2601e-02, -2.8542e-02,  6.7504e-03,  1.4272e-02,\n",
      "          4.7621e-03,  6.1300e-03,  2.6428e-02, -5.7039e-02,  1.6547e-02,\n",
      "          1.8853e-02,  4.5178e-03, -4.4403e-02, -2.5711e-02,  3.6453e-02,\n",
      "          2.3880e-02, -2.3196e-02,  4.9015e-02,  5.5322e-02,  6.0090e-02,\n",
      "          3.2918e-03, -2.3286e-02, -5.6556e-02, -3.9914e-02, -5.2595e-02,\n",
      "          3.6622e-02, -2.5178e-02,  4.2907e-02, -8.3352e-04,  3.7961e-02,\n",
      "         -1.1135e-02,  4.5996e-02, -2.8808e-02,  3.6123e-02, -4.7678e-02,\n",
      "         -1.4126e-02, -1.6181e-02, -5.6165e-02,  5.4369e-02, -1.5736e-02,\n",
      "          1.9981e-02, -1.3194e-02, -1.8812e-02,  5.7576e-02, -4.1195e-02,\n",
      "         -5.2307e-02,  1.8683e-03,  4.8242e-02, -6.7847e-03,  2.7957e-02,\n",
      "          3.6458e-02,  2.8348e-02,  3.7135e-04, -1.3033e-03,  5.3411e-02,\n",
      "         -2.8862e-02, -2.4137e-02,  5.5315e-02, -3.7528e-03, -2.7569e-02,\n",
      "          2.1500e-03,  1.2443e-03, -1.4323e-03, -4.5420e-02,  3.3519e-02,\n",
      "         -3.5637e-02]], device='cuda:0')), ('linear2.bias', tensor([-1.6756, -1.6636, -1.7044], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load(r'model/agent1.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent1.pth', 'agent2.pth']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
