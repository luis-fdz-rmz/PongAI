{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import K_w, K_UP, K_s, K_DOWN, QUIT, K_ESCAPE, KEYDOWN, KEYUP\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f65f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pong_ai_game:\n",
    "    def __init__(self, width_screen = 80, height_screen = 60):\n",
    "        self.width_screen = width_screen\n",
    "        self.height_screen = height_screen\n",
    "        self.score = 0\n",
    "        self.rect_ball_distance = [0.0,0.0]\n",
    "        self.screen_color = (35, 35, 35)\n",
    "        self.object_color = (251, 248, 243)\n",
    "        self.left_rect_color = (0,255,0)\n",
    "        self.right_rect_color = (0,255,0)\n",
    "        self.game_screen = pygame.display.set_mode((self.width_screen, self.height_screen))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.rect_speed = 3\n",
    "        self.render_game = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.score = 0\n",
    "        self.rect_ball_distance = [0.0,0.0]\n",
    "        self.left_rect = pygame.Rect(0, 7*self.height_screen//16, 2, self.height_screen//8)\n",
    "        self.right_rect = pygame.Rect(self.width_screen - 2, 7*self.height_screen//16, 2, self.height_screen//8)\n",
    "        self.ball_rect = pygame.Rect(self.width_screen//2, self.height_screen//2, 2,2)\n",
    "        self.ball_speed = [pow(-1, np.random.randint(0,2)), pow(-1, np.random.randint(0,2))]\n",
    "        self.render_game = False\n",
    "\n",
    "\n",
    "    def play_step(self, action):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "\n",
    "        paddle, move = action\n",
    "\n",
    "        #Move Up\n",
    "        if move == 0:\n",
    "            if paddle == 1 and self.left_rect.top > 0:\n",
    "                # Move rectangle 1 up by rectangle speed\n",
    "                self.left_rect = self.left_rect.move(0, -self.rect_speed)\n",
    "                if self.left_rect.top < 0:\n",
    "                    self.left_rect.top = 0\n",
    "            elif paddle == 2 and self.right_rect.top > 0:\n",
    "                # Move rectangle 2 up by rectangle speed\n",
    "                self.right_rect = self.right_rect.move(0, -self.rect_speed)\n",
    "                if self.right_rect.top < 0:\n",
    "                    self.right_rect.top = 0\n",
    "        #Move Down\n",
    "        elif move == 1:\n",
    "            if paddle == 1 and self.left_rect.bottom < 600:\n",
    "                # Move rectangle 1 down by rectangle speed\n",
    "                self.left_rect = self.left_rect.move(0, self.rect_speed)\n",
    "                if self.left_rect.bottom > self.height_screen:\n",
    "                    self.left_rect.bottom = self.height_screen\n",
    "            elif paddle == 2 and self.right_rect.bottom < 600:\n",
    "                # Move rectangle 2 down by rectangle speed\n",
    "                self.right_rect = self.right_rect.move(0, self.rect_speed)\n",
    "                if self.right_rect.bottom > self.height_screen:\n",
    "                    self.right_rect.bottom = self.height_screen\n",
    "        \n",
    "        #Get vertical distance between ball and rect\n",
    "        if self.ball_speed[0] < 0:\n",
    "            self.rect_ball_distance[0] = self.ball_rect.centery - self.left_rect.centery\n",
    "        elif self.ball_speed[0] > 0:\n",
    "            self.rect_ball_distance[1] = self.ball_rect.centery - self.right_rect.centery\n",
    "\n",
    "        self.game_screen.fill(self.screen_color)\n",
    "        self.ball_rect = self.ball_rect.move(self.ball_speed[0],self.ball_speed[1])\n",
    "\n",
    "        self.left_rectangle = pygame.draw.rect(self.game_screen, self.left_rect_color,self.left_rect)\n",
    "        self.right_rectangle = pygame.draw.rect(self.game_screen, self.right_rect_color,self.right_rect)\n",
    "        self.ball_rectangle = pygame.draw.rect(self.game_screen, self.object_color,self.ball_rect)\n",
    "\n",
    "        if self.render_game:\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)\n",
    "\n",
    "        self.ball_collision()\n",
    "\n",
    "        if self.illegal_ball():\n",
    "            return self.rect_ball_distance, True, self.score, self.get_state()\n",
    "\n",
    "        return self.rect_ball_distance, False, self.score, self.get_state()\n",
    "\n",
    "    def illegal_ball(self):\n",
    "        if self.ball_rect.left <= 0 or self.ball_rect.right >= self.width_screen:\n",
    "            return True\n",
    "    def ball_collision(self):\n",
    "        if self.left_rect.right == self.ball_rect.left:\n",
    "            if (self.left_rect.bottom >= self.ball_rect.centery >= self.left_rect.top):\n",
    "                self.ball_speed[0] = 1\n",
    "                self.score += 1\n",
    "        elif self.right_rect.left == self.ball_rect.right :\n",
    "            if (self.right_rect.bottom >= self.ball_rect.centery >= self.right_rect.top):\n",
    "                self.ball_speed[0] = -1\n",
    "                self.score += 1\n",
    "        if self.ball_rect.top <= 0 or self.ball_rect.bottom >= self.height_screen:\n",
    "            self.ball_speed[1] *= -1\n",
    "\n",
    "\n",
    "\n",
    "    def get_reward(self, agent, action, agent_obj):\n",
    "        # agent 1 is the left paddle, agent 2 is the right paddle\n",
    "        paddle_rect = self.left_rect if agent == 1 else self.right_rect\n",
    "        ball_y = self.ball_rect.centery\n",
    "\n",
    "        hit_ball_reward = 0\n",
    "        if agent == 1 and self.ball_rect.left <= self.left_rect.right and \\\n",
    "        self.left_rect.top <= self.ball_rect.centery <= self.left_rect.bottom:\n",
    "            hit_ball_reward = 2*self.score\n",
    "        elif agent == 2 and self.ball_rect.right >= self.right_rect.left and \\\n",
    "            self.right_rect.top <= self.ball_rect.centery <= self.right_rect.bottom:\n",
    "            hit_ball_reward = 2*self.score\n",
    "\n",
    "        # Penalize the agent for missing the ball (ball out of bounds)\n",
    "        miss_penalty = -5 if self.illegal_ball() else 0\n",
    "\n",
    "        # Action-specific reward: reward for appropriate paddle movement\n",
    "        action_reward = 0\n",
    "        wrong_move = False  # New flag\n",
    "\n",
    "        if action == 0:  # Move up\n",
    "            if ball_y < paddle_rect.centery:\n",
    "                agent_obj.right_action_count += 1\n",
    "                reward_multiplier = 1.15 ** agent_obj.right_action_count\n",
    "                action_reward = 0.2 * reward_multiplier\n",
    "                agent_obj.wrong_action_count = 0  # Reset on good action\n",
    "            else:\n",
    "                wrong_move = True\n",
    "\n",
    "        elif action == 1:  # Move down\n",
    "            if ball_y > paddle_rect.centery:\n",
    "                agent_obj.right_action_count += 1\n",
    "                reward_multiplier = 1.15 ** agent_obj.right_action_count\n",
    "                action_reward = 0.2 * reward_multiplier\n",
    "                agent_obj.wrong_action_count = 0\n",
    "            else:\n",
    "                wrong_move = True\n",
    "\n",
    "        # Apply exponential penalty if wrong action\n",
    "        if wrong_move:\n",
    "            agent_obj.wrong_action_count += 1\n",
    "            penalty_multiplier = 1.2 ** agent_obj.wrong_action_count  # decay factor\n",
    "            action_reward = -0.1 * penalty_multiplier\n",
    "            agent_obj.right_action_count = 0\n",
    "\n",
    "        # Combine all the rewards and penalties\n",
    "        total_reward = hit_ball_reward + miss_penalty + action_reward\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        screen = pygame.surfarray.array3d(self.game_screen)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor()])\n",
    "        screen = preprocess(screen)\n",
    "        return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85100d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "\n",
    "class MobileAgent:\n",
    "    def __init__(self, lr, rectangle, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "        self.model.classifier[1] = nn.Linear(1280, self.action_space)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.rectangle = rectangle  # 1 = left, 2 = right\n",
    "        self.model.to(device)\n",
    "        self.wrong_action_count = 0\n",
    "        self.right_action_count = 0\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0)  # Add batch dimension\n",
    "            logits = self.model(state)\n",
    "            action = torch.argmax(logits).item()\n",
    "        return action\n",
    "    def train_step(self, state, action, reward):\n",
    "        self.model.train()\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action = torch.tensor([action]).to(device)\n",
    "\n",
    "        logits = self.model(state)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        selected_log_prob = log_probs[0, action.item()]  # select log prob of the taken action\n",
    "\n",
    "        # Policy gradient loss: maximize reward -> minimize -log_prob * reward\n",
    "        loss = -selected_log_prob * reward\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luise\\AppData\\Local\\Temp\\ipykernel_2600\\1457354114.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  left_agent.model.load_state_dict(torch.load(r\"model\\2025_06_19_00_55_28_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Score: 9, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 35.5439,  Right Reward: 47.9336\n",
      "Episode 1, Score: 4, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 12.1478,  Right Reward: -4.2997\n",
      "Episode 2, Score: 13, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 40.3436,  Right Reward: 72.3049\n",
      "Episode 3, Score: 9, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: -4.4986,  Right Reward: 1.2346\n",
      "Episode 4, Score: 9, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 7.4033,  Right Reward: 19.7211\n",
      "Episode 5, Score: 9, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 40.7310,  Right Reward: 47.6513\n",
      "Episode 6, Score: 2, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 0.9325,  Right Reward: -2.1062\n",
      "Episode 7, Score: 1, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: 1.2561,  Right Reward: -0.1011\n",
      "Episode 8, Score: 3, Left Agent Loss: 0.0000, Right Agent Loss: 0.0000,  Left Reward: -96.1164,  Right Reward: 12.6090\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m right_action \u001b[38;5;241m=\u001b[39m right_agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Send actions to game: (paddle_id, move)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m game\u001b[38;5;241m.\u001b[39mplay_step((\u001b[38;5;241m1\u001b[39m, left_action))  \u001b[38;5;66;03m# Left paddle\u001b[39;00m\n\u001b[0;32m     38\u001b[0m distances, done, score, state_ \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mplay_step((\u001b[38;5;241m2\u001b[39m, right_action))  \u001b[38;5;66;03m# Right paddle\u001b[39;00m\n\u001b[0;32m     41\u001b[0m reward_left \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mget_reward(\u001b[38;5;241m1\u001b[39m, left_action, left_agent)\n",
      "Cell \u001b[1;32mIn[18], line 81\u001b[0m, in \u001b[0;36mpong_ai_game.play_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39millegal_ball():\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrect_ball_distance, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrect_ball_distance, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "Cell \u001b[1;32mIn[18], line 157\u001b[0m, in \u001b[0;36mpong_ai_game.get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m screen \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39marray3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_screen)\n\u001b[0;32m    153\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    154\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[0;32m    155\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m    156\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m--> 157\u001b[0m screen \u001b[38;5;241m=\u001b[39m preprocess(screen)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m screen\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_pil_image(pic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\torchvision\\transforms\\functional.py:324\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(npimg, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\PIL\\Image.py:3338\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3335\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires either tobytes() or tostring()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 3338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, rawmode, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\PIL\\Image.py:3240\u001b[0m, in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3237\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[1;32m-> 3240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\PIL\\Image.py:3177\u001b[0m, in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m   3175\u001b[0m         decoder_args \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m-> 3177\u001b[0m     im\u001b[38;5;241m.\u001b[39mfrombytes(data, decoder_name, decoder_args)\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\luise\\anaconda3\\envs\\MachineAI\\Lib\\site-packages\\PIL\\Image.py:873\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[1;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m    871\u001b[0m d \u001b[38;5;241m=\u001b[39m _getdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, decoder_name, decoder_args)\n\u001b[0;32m    872\u001b[0m d\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[1;32m--> 873\u001b[0m s \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mdecode(data)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    876\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot enough image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Initialize game\n",
    "pygame.init()\n",
    "game = pong_ai_game(120,90)\n",
    "left_agent = MobileAgent(lr=1e-4, rectangle=1, action_space=2)\n",
    "right_agent = MobileAgent(lr=1e-4, rectangle=2, action_space=2)\n",
    "\n",
    "# left_agent.model.load_state_dict(torch.load(r\"model\\2025_06_19_00_55_28_model.pth\"))\n",
    "# Optionally share initial weights\n",
    "right_agent.model.load_state_dict(left_agent.model.state_dict())\n",
    "\n",
    "n_episodes = 5000\n",
    "for episode in range(n_episodes):\n",
    "    game.reset()\n",
    "    done = False\n",
    "    training = False\n",
    "    total_reward = [0, 0]  # For left and right agents\n",
    "    left_agent_loss = 0.0\n",
    "    right_agent_loss = 0.0\n",
    "    if episode%50 == 0:\n",
    "        game.render_game = True\n",
    "\n",
    "    while not done:\n",
    "        state = game.get_state().to(device)\n",
    "        if game.score >=10 and training:\n",
    "            print(\"EVALUATING\")\n",
    "            training = False\n",
    "            left_agent.model.eval()\n",
    "            right_agent.model.eval()\n",
    "        left_action = left_agent.act(state)\n",
    "        right_action = right_agent.act(state)\n",
    "\n",
    "        # Send actions to game: (paddle_id, move)\n",
    "        game.play_step((1, left_action))  # Left paddle\n",
    "        distances, done, score, state_ = game.play_step((2, right_action))  # Right paddle\n",
    "\n",
    "\n",
    "        reward_left = game.get_reward(1, left_action, left_agent)\n",
    "        reward_right = game.get_reward(2, right_action, right_agent)\n",
    "        total_reward[0] += reward_left\n",
    "        total_reward[1] += reward_right\n",
    "\n",
    "        # Train both agents\n",
    "        if training:\n",
    "            left_loss = left_agent.train_step(state, left_action, reward_left)\n",
    "            right_loss = right_agent.train_step(state, right_action, reward_right)\n",
    "\n",
    "            # Track and accumulate losses\n",
    "            left_agent_loss += left_loss\n",
    "            right_agent_loss += right_loss\n",
    "\n",
    "        if left_agent.wrong_action_count > 0:\n",
    "            game.left_rect_color = (255,0,0)\n",
    "        else:\n",
    "            game.left_rect_color = (0,255,0)\n",
    "        if right_agent.wrong_action_count > 0:\n",
    "            game.right_rect_color = (255,0,0)\n",
    "        else:\n",
    "            game.right_rect_color = (0,255,0)\n",
    "\n",
    "\n",
    "    if episode % 50 == 0 and episode != 0:\n",
    "        rn = datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        # Use os.path.join to create a valid file path\n",
    "        file_path = os.path.join(\"model\", f\"{rn}_model.pth\")\n",
    "        \n",
    "        # Save the model's state_dict (weights) at that point\n",
    "        torch.save(left_agent.model.state_dict(), file_path)\n",
    "        right_agent.model.load_state_dict(left_agent.model.state_dict())\n",
    "        print(f\"Model saved at {file_path}\")\n",
    "    game.render_game = False\n",
    "    # Print average loss per episode for both agents\n",
    "    print(f\"Episode {episode}, Score: {score}, Left Agent Loss: {left_agent_loss:.4f}, Right Agent Loss: {right_agent_loss:.4f},  Left Reward: {total_reward[0]:.4f},  Right Reward: {total_reward[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
